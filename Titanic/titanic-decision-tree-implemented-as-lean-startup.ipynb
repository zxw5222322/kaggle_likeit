{"cells":[{"metadata":{"_cell_guid":"afc5b9fc-85fa-4422-9fd6-3b9a814cafef","_uuid":"8993d12a1fc7e7f97e0ffd7512d4fa08139f7ec1"},"cell_type":"markdown","source":"Usually when we want to learn a new technology, we read about a Hello World version of it. I think Titanic can be the Hello World for Machine Learning <br> <br>\nYou can find plenty of good blogs and tutorial about Titanic and ML on internet. Even on kaggle you can find very good kernels describing the whole ML solution. <br>\nI have to say that I really like the concept of kernels. You can learn/see a lot of new ideas and solutions from other experimented people. For me, in comparison with a blog post, the biggest plus for kernels is that I can see the whole code and if I want I can fork it and change/execute it.\n\nMajority of kernels I read present the final solution, but a ML project has many cycles of development until the final version, if there is one. That's why I would like to show you a different way of writing a kernel. <br>\n\nTitanic requires a classification model. I chose one of the simplest one, DecisionTreeClassifier from sklearn. <br>\nThe entire kernel is around three cycles of development. <br>\nIn the **first cycle** I will get used with the dataset, make some visualizations and see if decision tree model can obtain better than random results.  \nIn the **second cycle** I will try different hyperparamentes for the model with the hope to improve the score on the leaderboard.\nBased on the knowledge from the previews two cycles, in the **third cycle**  I will add more features to train the model(in combination with different hyperparamenters), also with the goal of obtaining a better score. <br>\n\nI like to implement each cycle of development in three phases (similar with lean startup concept):\n    1. ideas - assumptions about what can improve the score\n    2. implement - transform ideas into code \n    3. evaluate - evaluate the results of the model\n\nLean startup is about how to build a product. It's funny that I found its usability in creating this kernel. <br>\nIf you want to read about the basic principles of learn startup you can start from [here](http://http://theleanstartup.com/principles) or you can read the entire [book](http://http://theleanstartup.com/book). \n\n**Just for fun, I created a web application which shows your chance to survive on Titanic. <br>\nYou can play with it here : http://survivortitanic.com**\n\n\n\n\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"2eb0f3d2-9dfa-4edf-ada3-4ad3281f05dd","_uuid":"1984aa1f25c0323c5595ecd4e027f90b213d93ca","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"35da0960-218a-46c2-b62f-e7899174ef1b","_uuid":"a61c81e75878a321cca568cf328fd2617d9d0d66","collapsed":true},"cell_type":"markdown","source":"# 1. Cycle one\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ce83ecbc-b564-4990-9c98-5e6e976e15a2","_uuid":"16797c0aef03c54d4c922bc45fd2fa92d3bab203"},"cell_type":"markdown","source":"## 1.1 Ideas\nThe main goals of this cycle is to get used with the dataset, get insights from data visualizations and see if we can have more than random predictions using the simplest decision tree model possible.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"6197f2a0-a99c-4e64-85ba-576ccf1f6e01","_uuid":"b2b4c8d5293ce0e088bf4bd51f5785222b79d6de","collapsed":true,"trusted":true},"cell_type":"code","source":"train_titanic = pd.read_csv(\"../input/train.csv\")\ntest_titanic = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"53a220f8-0021-4925-8797-de33875f991e","_uuid":"cc485eaa0d2295b51105ec55b1e4585867d2948d"},"cell_type":"markdown","source":"### Few stats about the datasets\n#### training set","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"db3c30cd-928c-4bac-8d5c-47e3043f726a","_uuid":"67c8847a97434e0e82263c56e24fccdbedeeb168","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"print(\"(# of rows, # of columns) \" + str(train_titanic.shape))\ntrain_titanic.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"33c096c4-0d54-41ab-9df9-579bf76752bd","_uuid":"08c02c696497f77841baf678f8d030900fa54c73"},"cell_type":"markdown","source":"Training set contains in total 891 exemples. <br>\nFrom the above tables we can see that we deal with measing data, like age and cabin. We will deal with missing data bellow in the notebook.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"7d8eec6c-d54f-4390-b057-be1c67de44f6","_uuid":"ba1a4556a5c66b6480ed069c80e0e872fd3f2371"},"cell_type":"markdown","source":"#### test set","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f277ad62-6af7-483c-b030-fff0a5a199b0","_uuid":"24410e292cc28b876e455c52eb903b11f91d50f7","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"print(\"(# of rows, # of columns) \" + str(test_titanic.shape))\ntest_titanic.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e79786d5-8e3d-4e67-aeb6-9f68fd501859","_uuid":"f0774d6625f5a2ff1cded99b8c2a60ff21026ba0"},"cell_type":"markdown","source":"Test set contains in total 418 examples. <br>\nHere, we are also dealing with missing data in columns like age, fare and cabin.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a608df04-c7d0-456a-bce3-3f3be9602e89","_uuid":"c934442ef9a661652ed314ff87d04aa94f204599","collapsed":true},"cell_type":"markdown","source":"### First visualizations\nLet's see how balanced are the classes from our target variable : Survived","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b4af9cca-94ec-4ca1-8e52-4830482193c8","_uuid":"d4ebbca82b6e64ac226fde2a89ed29f1235a636b","collapsed":true,"trusted":true},"cell_type":"code","source":"train_titanic.groupby([\"Survived\"])[\"Survived\"].count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e9eebf76-5b5c-4115-9179-3d52cb4edcc2","_uuid":"8d919d493fae74301218d928077d02db09472d97"},"cell_type":"markdown","source":"From above histogram we can see that the number of people who survived are unfortunetely lower that those who died. <br>\n0 = No, 1 = Yes\n<br>\nMain reasons for death were : (info from http://www.eszlinger.com/titanic/titanfacts.html): <br>\n        * 2,208 lifeboat seats were needed and only 1,178 lifeboat seats were carried aboard.\n        * One of the first lifeboats to leave the Titanic carried only 28 people; it could have held 64 people.\n        * Very few people actually went down with the ship. Most died and drifted away in their life-jackets.\n        \n        ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"5b70f688-d426-400e-b15f-c90211203613","_uuid":"365c61a9978bcc2a1a91dc6387d61f8e3615fc48","collapsed":true},"cell_type":"markdown","source":"### Feature correlations","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"af456c6d-f824-4621-8185-3e606bb18468","_uuid":"c49bc63b8bea5937647589e1e14a833de76a8fbe","collapsed":true,"trusted":true},"cell_type":"code","source":"import seaborn as sns\ntrain_corr = train_titanic.corr(method=\"spearman\")\nplt.figure(figsize=(10,7))\nsns.heatmap(train_corr, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a832d52e-2d66-481e-8810-554b9c2e3200","_uuid":"76e8d649bce842606d5f0ffd5423f0e38bf73987","collapsed":true},"cell_type":"markdown","source":"As we can see from the above heatmap, there is no strong correlation between feature variables.\nIt is a good news, it means that all features will have an individual importance to predict the target variable. \n\nIt is also good because we have a training dataset of small size. When you have a small training set with many correlated features, it means that in the end you have fewer features to reflat the reality and also it is prone to overfiting.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c2ce1a77-28a3-4bb1-b6e9-257251922506","_uuid":"70eca61321c845b02a4bee81308b85dee1c412ab"},"cell_type":"markdown","source":"## 1.2 Implement","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"5d2cb99c-eeb0-4df6-8ccf-7f76e63a1979","_uuid":"8aed241bfea9b7b86e302385d32cec0ef856d2b8"},"cell_type":"markdown","source":"### Give it a try with Decision Tree","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"41b37005-ce53-4fb0-a324-fc259359e5bd","_uuid":"edb1efe87fcc495c745b90e527172a78bf71e759","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3318d650-4fe3-4a07-9c5a-63a9d51277e1","_uuid":"406542b3977feab815c88164d0e9ff1c9dbfa1c8"},"cell_type":"markdown","source":"Let's start the easiest way possible. <br>\nStart with the most relevant set of features which also don't contain missing values. <br>\nFor me, the most relevant ones would be Pclass, Sex and Fare\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b4b77142-b3ab-4e4c-b632-4dd994d4ab4c","_uuid":"0cded3297da51705a04b14e14d71a5b33e28004f","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"print(train_titanic.shape) \ntrain_titanic[[\"Pclass\", \"Sex\", \"Fare\"]].describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"63227e33-ccfb-439c-afa9-31c149a4aa7f","_uuid":"c78d52f88c95e99ab69a2e0aaf4826a97b4ba893"},"cell_type":"markdown","source":"#### Feature visualization","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1f8eb740-65eb-4e1c-9358-14f8120b5b9c","_uuid":"8594e2b82b14582f8c87ef23a0416ddf3e1350eb"},"cell_type":"markdown","source":"#### Sex feature","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"30c50fd2-995a-4b17-b8b6-171d4b47a5f5","_uuid":"3e6d9e366c6be7ba9878ad3f10895d5e97307535","collapsed":true,"trusted":true},"cell_type":"code","source":"train_titanic.groupby(\"Sex\")[\"Sex\"].count().plot.bar(x=\"Sex\", title=\"Sex feature distribution\")\n\n# for the next charts I want to represent the features related to the target variable, survived. \n# Because of this I will create two new columns, alive, not_alive\ntrain_titanic[\"Alive\"] = train_titanic[\"Survived\"].apply(lambda s : s )\ntrain_titanic[\"Not_alive\"] = train_titanic[\"Survived\"].apply(lambda s : abs(1 - s))\ntrain_titanic.groupby([\"Sex\"])[\"Alive\", \"Not_alive\"].sum().plot.bar(title=\"Sex feature distribution related to target variable\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8b0d6f23-2684-4180-ade9-66e61ac76ffd","_uuid":"9537f80fb0a27b97a9ffa2a872e92cc4d9c84e8d"},"cell_type":"markdown","source":"From the first chart we can see that on titanic was almost double males than females. From the second chart we can see that the chance to survive for a men is more lower than for a female. <br>\nBased on these charts, the sentance [Women and children first](https://en.wikipedia.org/wiki/Women_and_children_first) seems to be true.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"57ed293a-5df8-4741-94ab-469c0a2e4856","_uuid":"9388f7eaa4509f67bb36dacc4ed40b90935cf120"},"cell_type":"markdown","source":"#### Pclass feature","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e98e5d9e-2393-4cf3-9232-598e8736c147","_uuid":"12343120cc0a1f0334d0179786464bac52a28141"},"cell_type":"markdown","source":"Based on kaggle description, Pclass represent the ticket class or in other works the socio-economic status","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f1ec0e30-0d6c-4354-915b-3c3d084b9982","_uuid":"deabe1acd5de15909fb5ab4fe705fac3d765488c","collapsed":true,"trusted":true},"cell_type":"code","source":"train_titanic.groupby([\"Pclass\"])[\"Pclass\"].count().plot.bar(title=\"Distribution of Pclass feature\")\ntrain_titanic.groupby([\"Pclass\"])[\"Alive\", \"Not_alive\"].sum().plot.bar(title = \"Pclass feature distribution related to target variable\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aa5491b5-4c22-4d2e-812f-a4edee2ef662","_uuid":"cfff639339efd8b7b753f8e6a5a39066934c0252"},"cell_type":"markdown","source":"It seems that on titanic was way more 'poor' people than 'rich' ones. <br>\nThe second chart shows that rich people (Pclass=1) had a bigger chance to survive. Somehow it reflec the reality, because in general rich people are more influential.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"717377eb-214c-4b30-85e8-32cf9c10d093","_uuid":"81d26869fcfa44c69681fc94da51349bfbe16830"},"cell_type":"markdown","source":"### Fare features","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b03a93d1-4586-4ba7-8a8b-768f6e67f554","_uuid":"bdbc1a8b5787fd1d18e956ba0d0799473189e288"},"cell_type":"markdown","source":"Fare feature represent the ticket price each passanger paid. <br>\nBecause fare is a continous feature, I will create fare categories to better visualize it.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4fada85c-3bb8-4005-81f4-223d4b368313","_uuid":"e582920df541134c5caa51398e181c04e37b7d63","collapsed":true,"trusted":true},"cell_type":"code","source":"bins = list(range(0, 110, 10))\nbins.append(600)\ntrain_titanic[\"Fare_category\"] = pd.cut(train_titanic.Fare, bins=bins).apply(lambda x : x.right)\ntrain_titanic.groupby([\"Fare_category\"])[\"Alive\", \"Not_alive\"].sum().plot.bar(figsize=(15,5))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fad0a78c-9572-44fc-b259-7bad71952fd4","_uuid":"3795c3e6d417f851ce6354ca8e6255d219f00e61"},"cell_type":"markdown","source":"From the above chart, it seems if you would buy an expensive ticket you would have more change to survive. Somehow this visualization reflect the results from Pclass visualizations.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4e5b0605-89c6-4911-b6be-f16130da9a70","_uuid":"1b9c78dd488d8b7196ae3c8ca74dc030b0700b37","collapsed":true,"trusted":true},"cell_type":"code","source":"train_titanic[[\"Fare\"]].plot.box(vert=False, figsize=(15,5))\nprint (\"Mean, median fare \" + str(train_titanic[\"Fare\"].mean()) + \", \" + str(train_titanic[\"Fare\"].median()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6b62de26-fcf6-4610-9bcf-a4c4ed02f21b","_uuid":"2444cbe76ec5cde9870f58f1910fff5a0dd93b4a"},"cell_type":"markdown","source":"Based on above boxplot, we have some outliers in our training set. This can be due to data errors or maybe some passangers paid a lot more than the majority.  <br>\nA good practice is to remove the outliers from the training set, but due to the very small size of titanic training set, it is not such an easy decision. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"afa5534b-5d0b-4ed4-943f-94334c700a66","_uuid":"337c65559ff673ece4754d78c560e115f2cf9ab9"},"cell_type":"markdown","source":"### Utility methods","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"9fd6fd5b-e85b-470c-82c3-a590f74b91b5","_uuid":"19d62d08cc7c7155954a4a0a0db4d5be03e6906d","collapsed":true,"trusted":true},"cell_type":"code","source":"import graphviz \nfrom sklearn.tree import export_graphviz\n\ndef plot_decision_tree(decision_tree, features_names) :\n    dot_data = export_graphviz(decision_tree=decision_tree, out_file=None, feature_names=features_names)\n    return graphviz.Source(dot_data)\n\ndef display_feature_importance(model, cols):\n    featureImportance = pd.Series(model.feature_importances_, index=cols).sort_values(ascending=True)\n    featureImportance.plot(kind=\"barh\")\n\ndef resume_wrong_predictions(train_titanic, predictions, groupby_col = [\"Fare_category\", \"Sex\", \"Pclass\"]) :\n    wrong_predictions = train_titanic[train_titanic[\"Survived\"] != predictions] \\\n        .groupby(groupby_col)['Survived'] \\\n        .agg(['count'])   \n    training_predictions = train_titanic. \\\n        groupby(groupby_col)[\"Survived\"]. \\\n        agg([\"count\"])  \n    results = pd. \\\n        merge(wrong_predictions, training_predictions, left_index=True, right_index=True). \\\n        rename(columns={\"count_x\" : \"wrong_label_count\", \"count_y\" : \"training_label_count\"} )\n    results[\"wrong_prediction_percetage\"] = (100 * results[\"wrong_label_count\"]) / results[\"training_label_count\"]\n    return results\n\ndef save_submition_file(filename, passengerId, predictions) :\n    kaggle_test_submition = pd.DataFrame({\"PassengerId\":passengerId, \"Survived\":predictions})\n    kaggle_test_submition.to_csv(filename, index=False)\n    \ndef feature_imputer_titanic(column, imputer_column, missing_values=\"NaN\", strategy=\"median\"):\n    imputer = Imputer(missing_values=missing_values, strategy=strategy).fit(train_titanic[[column]])\n    train_titanic[imputer_column] = imputer.transform(train_titanic[[column]])\n    test_titanic[imputer_column] = imputer.transform(test_titanic[[column]])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d752c09-cf60-4852-aeac-7ab04f4e3bc4","_uuid":"f5f8973c6abb031641f0c22c0a9ec8336174a730"},"cell_type":"markdown","source":"### Implement first version of Decision Tree","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f48cb1ff-2caa-4a8e-839f-fffadbdf9f27","_uuid":"5b3596786d2bb0d68f2d02d2801e09313cac9b49"},"cell_type":"markdown","source":"Sex feature is categorical and we need to tranform it into numerical for DecisionTree model.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"adca054d-904e-46fd-9aaf-113f090af82a","_uuid":"0b6e59c4dea3be26cf90837d19a2e2a3f2c4d559","collapsed":true,"trusted":true},"cell_type":"code","source":"sexLabelEncoder = LabelEncoder()\nsexLabelEncoder.fit(train_titanic[\"Sex\"])\ntrain_titanic[\"Sex_encoded\"] = sexLabelEncoder.transform(train_titanic[\"Sex\"])\ntest_titanic[\"Sex_encoded\"] = sexLabelEncoder.transform(test_titanic[\"Sex\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1d143929-770a-463a-9908-09b345dbd517","_uuid":"983f473d154207452c16fd38fa680028255c5557"},"cell_type":"markdown","source":"The kaggle test set contains in total 418 examples and \"Fare\" features has one missing value.  <br>\nAll sklearn models are wainting the features to be all numeric and contain no missing values. Because of the fare missing value, the decision tree predict method will fail. <br>\n0ne way to handle missing values in sklearn is using Imputer which imput the missing values using either mean, median or most frequent value of the column.\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"01313f54-6e5d-423e-bd5c-0fa185789ecd","_uuid":"7c325a6c893a6b0f80f6e3d929a1f28d74b25f6b","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import Imputer\n\nfare_imputer = Imputer(missing_values='NaN', strategy=\"median\").fit(test_titanic[[\"Fare\"]])\ntest_titanic[\"Fare_median\"] = fare_imputer.transform(test_titanic[[\"Fare\"]])\ntrain_titanic[\"Fare_median\"] = fare_imputer.transform(train_titanic[[\"Fare\"]])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a66897e2-f940-4c4c-980b-9879c378b178","_uuid":"1538c2d63a66bdcecc1df63e15b187d63c2decf4"},"cell_type":"markdown","source":"Because the training set is so small I would like to use it all to train the model. I will check the performance of the model directly on kaggle test set (leaderboard). Lucky us that we have 10 submits/day.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"7460f569-9365-4488-a6e3-788dc3e6ba85","_uuid":"cedc0a828bd26b5b4a4b5c915419395f0735b2fa","collapsed":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"dt_col_cycle1_0 = [\"Pclass\", \"Sex_encoded\", \"Fare_median\"]\ndt_model = DecisionTreeClassifier(random_state=1987)\ndt_model.fit(train_titanic[dt_col_cycle1_0], train_titanic[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e4db9a89-e144-4aa0-8057-a5230501667a","_uuid":"443dc32f02168e6e6e465562ffe52ed483f163fb"},"cell_type":"markdown","source":"## 1.3 Evaluate","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"bf12db1c-7970-4832-89d6-08168b21ff6c","_uuid":"f7e095723198f11aa268e2b0d69821bdaeb7e085"},"cell_type":"markdown","source":" Our first submit to kaggle","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"031e2b31-9338-404e-b152-bc1c8a49e57c","_uuid":"c432345a23f0ccabafaa819aed7c06a678e34b44","collapsed":true,"trusted":true},"cell_type":"code","source":"dt_col_cycle1_0","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"97d22259-163d-48cd-b884-33d372c02b08","_uuid":"84fec968db5bc26585bf095a6ad169f74376e85a","collapsed":true,"trusted":true},"cell_type":"code","source":"test_titanic_predictions = dt_model.predict(test_titanic[dt_col_cycle1_0])\nsave_submition_file(\"dt_cycle_1_submission.csv\", test_titanic[\"PassengerId\"], test_titanic_predictions )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bb95eb49-1043-4ebe-9e0c-6691b38dff64","_uuid":"ede757f7170f053eaa8241ef86c17ffdea1c20b6"},"cell_type":"markdown","source":"yeeey, we obtained 0.77033 accuracy score on kaggle public leaderboard which is a promising step !","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b46928f9-527a-4bf7-be94-9a2abe313157","_uuid":"175a4d1d8302795007555dc8885ef8e92f188c64"},"cell_type":"markdown","source":"Bellow you can see the features importance of the model.\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"3541efc2-7db4-44b4-b2a5-20adf17fe75d","_uuid":"2148c9c734e27ed7c30bc4ea7594b053b7d0e299","collapsed":true,"trusted":true},"cell_type":"code","source":"display_feature_importance(dt_model, dt_col_cycle1_0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5aa06103-0274-4140-9a54-dc585c1fb3e1","_uuid":"6ecf6b0d952351f545b8336a9e19fe9ff6022eef"},"cell_type":"markdown","source":"Let's see where we get wrong predictions using the training set predictions. <br>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"2b377884-534e-4bc0-a493-d8c72b8db54b","_uuid":"0270d9513af8db023e399e2cd525ba3b431b0e46","collapsed":true,"trusted":true},"cell_type":"code","source":"resume_wrong_predictions(train_titanic, dt_model.predict(train_titanic[dt_col_cycle1_0]), groupby_col=[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"965fbf6d-a9d6-456d-97a7-4879eb6d1710","_uuid":"a69ec44cea2caccbdbe90385d5edcedcd7212783","collapsed":true,"trusted":true},"cell_type":"code","source":"# I want to see the wrong predictions based on the features the model was trained and also taking \n# in considerence the feature importance\nresume_wrong_predictions(train_titanic, dt_model.predict(train_titanic[dt_col_cycle1_0]), groupby_col=[\"Fare_category\", \"Sex\", \"Pclass\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0b7c2e23-b5d6-4783-b0da-4326435d999b","_uuid":"39d8b05f3c298cbea9af9c1d20a500768394c73c"},"cell_type":"markdown","source":"I cannot see a clear pattern where our model makes wrong predictions based on the above table. This can be true because surviving on Titanic was also a lucky situation.  <br>\n\nIf you can see a pattern, I will be very  happy if you will let a comment.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c8ef52e7-bf8e-4bd6-aa84-bec6e61a1fe1","_uuid":"f33d4b232231db0f2ba216d94482ec694761a389"},"cell_type":"markdown","source":"All the predictions were made based on a single decision tree. Let's see how it looks !","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ec92c900-ec36-4d75-a670-72d6509b87bb","_uuid":"bbf1277cb856092badb8a81f858b34f8614091c6","collapsed":true,"trusted":true},"cell_type":"code","source":"plot_decision_tree(dt_model, dt_col_cycle1_0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef787caf-3c69-4749-a82f-0295aec712ea","_uuid":"feb2f522b9d1a35b897e2b5cf60351f88b5c9668"},"cell_type":"markdown","source":"That was it for the first cycle. <br>\nIt's a pretty good score using the default decision tree hyperparameters and a small subset of features. <br>\nLet's see if we can improve the accuracy tunning some of the hyperparameters!","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"29431bff-d78b-45e6-9950-f7b0e7ab2c78","_uuid":"7cf754e370f95a7dd8cf0c3ce50adb09ad9e8b3d"},"cell_type":"markdown","source":"# 2. Cycle two\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b46cf0e9-132f-43d8-884e-ed64b8a3037c","_uuid":"526800975650a5537c941e5cb8fab41bae274525","collapsed":true},"cell_type":"markdown","source":"## 2.1 Ideas, assumptions\nI suppose the score can be improved using the same set of features and only tuning decision tree hyperparameters. <br>\nIn the first cycle we initialized decision tree with its default parameters. This can cause over-complex trees which don't generalize well. <br>\nSklearn implementation of decision tree offers us multiple options of pruning to avoid such problems :\n    * setting max_depth\n    * setting min_samples_leaf\nYou can find docs about above parameters on sklearn decision tree classifier page : http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n    \n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"5f692fa4-f0c2-4f97-8947-8717b9f6c26c","_uuid":"8b3297f7dc373db0b9f87443481fededce8c4b73"},"cell_type":"markdown","source":"## 2.2 Implement","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b89d2f81-9b4d-421b-9b4f-3e1732b49f3e","_uuid":"f7a86f0e9eddadfe50e75cd6d955f5aef8820b69"},"cell_type":"markdown","source":"Let's play with different values for max_depth hyperparametres <br>\n\nSklean docs for max_depth <br>\n*max_depth : int or None, optional (default=None) <br>\nThe maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.*","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"676efcb4-7385-418e-a193-7f2c1385fdf4","_uuid":"0e581a588f2a2bcffc5dd89de8ab4cff9952a26a","collapsed":true,"trusted":true},"cell_type":"code","source":"max_depth = 6\ndt_depth_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=max_depth, random_state=1987)\ndt_depth_model.fit(train_titanic[dt_col_cycle1_0], train_titanic[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc8538ed-3f26-48a5-8119-b2f4fb62f420","_uuid":"4574fce3a5d03ac660c1f99b8eaadc8251b4e727","collapsed":true,"trusted":true},"cell_type":"code","source":"test_titanic_predictions = dt_depth_model.predict(test_titanic[dt_col_cycle1_0])\nsave_submition_file(\"dt_depth_6_submission.csv\", test_titanic[\"PassengerId\"], test_titanic_predictions)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"66c66dbc-7f1f-4c46-82b5-82e6be8c7bdb","_uuid":"ee25bd1b1194307f231ebf16d646cf3ceb75dff5"},"cell_type":"markdown","source":"The best kaggle test set accuracy was obtained by max_depth 6 ! <br>\nBellow you can see the accuracy of the model trained with different max_depth values :  <br>\nmax_depth 4 -> 0.78468 <br>\nmax_depth 5 -> 0.78468 <br>\nmax_depth 6 -> 0.79425 <br>\nmax_depth 7 -> 0.77033 <br>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"267498e2-9bf8-408f-bd0d-b2598670f1da","_uuid":"e3e85e0516d5492133343b10c93f2ec17e7c2119"},"cell_type":"markdown","source":"Try multiple values for min_samples_leaf <br>\n\nSklean docs for min_samples_leaf : <br>\n*min_samples_leaf : int, float, optional (default=1) <br>\nThe minimum number of samples required to be at a leaf node: <br>\n    If int, then consider min_samples_leaf as the minimum number. <br>\n    If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.*\n\n\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"0e583132-3136-48d1-a278-ddfb5cbca42d","_uuid":"e605c6e961dd93855bbc6d17b3b91c1d71308a0a","collapsed":true,"trusted":true},"cell_type":"code","source":"min_samples_leaf = 10\ndt_leaf_model = DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf=min_samples_leaf, random_state=1987)\ndt_leaf_model.fit(train_titanic[dt_col_cycle1_0], train_titanic[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b92059cb-5943-4681-babd-80efb5004513","_uuid":"45183e8296d877b837e22c74deaad60fcfd81a4f","collapsed":true,"trusted":true},"cell_type":"code","source":"test_titanic_leaf_predictions = dt_leaf_model.predict(test_titanic[dt_col_cycle1_0])\nsave_submition_file(\"dt_min_samples_leaf_10_submission.csv\", test_titanic[\"PassengerId\"], test_titanic_leaf_predictions)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"99d31330-6ff4-49f7-ac3f-4713e1767313","_uuid":"58bc410fe2b8cb6b1f2ef32892dcbd0b2f98c37a"},"cell_type":"markdown","source":"The best accuracy was obtained using min_sample_leaf = 10 <br>\nInteresting that the accuracy for min_sample_leaf = 10 is the same for max_depth = 6 <br>\nmin_sample_leaf 5  -> 0.76076 <br>\nmin_sample_leaf 10 -> 0.79425 <br>\nmin_sample_leaf 15 -> 0.76555 <br>\nmin_smaple_leaf 20 -> 0.77511 <br>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"d9766c65-5e09-4fb2-aa44-e19dbb34fd3a","_uuid":"c9a171ad98425d3d706f4f45a14c1f48d45157f1"},"cell_type":"markdown","source":"Try the class_weight hyperparameter with value balanced.\n\n*The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))*","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ea39fcc3-b823-4d71-99c3-db80ecc95f68","_uuid":"0af3f456a2780c316b247edf26fe628826affd4b","collapsed":true,"trusted":true},"cell_type":"code","source":"dt_weigth_model = DecisionTreeClassifier(random_state=1987, min_samples_leaf=10, class_weight=\"balanced\")\ndt_weigth_model.fit(train_titanic[dt_col_cycle1_0], train_titanic[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3f3dda62-970a-4599-aedd-e7b65d5eb53a","_uuid":"1015216664916d02da01b1218dda2b298f101125","collapsed":true,"trusted":true},"cell_type":"code","source":"test_titanic_weight_predictions = dt_weigth_model.predict(test_titanic[dt_col_cycle1_0])\nsave_submition_file(\"dt_weight_balanced_submission.csv\", test_titanic[\"PassengerId\"], test_titanic_weight_predictions)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"07c1bddd-de5b-4286-8d2b-1190f7e01f1a","_uuid":"24ddfe0f7381544983c5c9abb55b65682e52ee22"},"cell_type":"markdown","source":"The accuracy for class_weight=balanced and min_samples_leaf = 10 was 0.75598 which is not an improvement.\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a2260232-d2ca-4694-b717-78c2249ac712","_uuid":"6b5b9967bb4ea1c9295eec4e5b8b146c17853153"},"cell_type":"markdown","source":"## 2.3 Evaluate","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"097d1149-c5f4-496b-a933-93caf6b8569a","_uuid":"50d3f5e3306a5a3fe558ea1506da8003c79adf92"},"cell_type":"markdown","source":"In the first cycle we obtained accuracy = 0.77033. <br>\nJust tuning the hyperparameters we improve the accuracy to 0.79425 using min_sample_leaf = 10 ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"9e5d1228-edb9-49ca-ac2f-6c8b263cbd0e","_uuid":"0a522688b3e1e873dbb49fddeb38ca975aa50396"},"cell_type":"markdown","source":"Let's visualize which are the most influential features for our best decision tree model using min_sample_leaf = 10. <br>\nAs you can see there are some differences compared with the earlier feature importance plot. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"890eabf1-891a-46c1-b42c-6959ec2d2893","_uuid":"18c9b9bb83d7942068121eb8cceb29abcd4549e4","collapsed":true,"trusted":true},"cell_type":"code","source":"display_feature_importance(dt_leaf_model, dt_col_cycle1_0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"89952b23-9029-46b5-8521-cd79d0d6901a","_uuid":"2e8f5b9b8f049831b03ea45b21d4df0667742a3e","collapsed":true,"trusted":true},"cell_type":"code","source":"training_predictions = dt_leaf_model.predict(train_titanic[dt_col_cycle1_0])\nprint(\"train accuracy : \" + str(accuracy_score(train_titanic[\"Survived\"], training_predictions)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1506416a-5937-4b02-b5e7-f2a4607e092f","_uuid":"33e83f5f60e04c260fc86bedee1e30ff0422d5e8","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"# I want to see which class has more wrong predictions\nresume_wrong_predictions(train_titanic, training_predictions, groupby_col=[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8366fe24-b4d1-4de4-833c-87548782cc95","_uuid":"aeb89dc7f2cba113acd369c1807a9fbff05b4480"},"cell_type":"markdown","source":"Something interesting is happening. <br>\nIn the first cycle we had a total of 85 wrong **training predictions** compared with 146 wrong predictions from the cycle 2, even if the accuracy on kaggle test set is bigger for cycle 2. Do you have any guesses why ? :)\n\n\n\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"5ba0480d-42ac-41b4-bfa4-c53e8e813316","_uuid":"35a7d10ec8f06a390d706e65f78b3b1cffeb11e3","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"# I want to see the wrong predictions based on the features the model was trained and also taking \n# in considerence the feature importance\nresume_wrong_predictions(train_titanic, training_predictions, groupby_col=[\"Sex\", \"Fare_category\", \"Pclass\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ec0d7d81-d52f-4251-9bab-b6ab3b5827e4","_uuid":"ae76540b555a4ca1112fbf7601c032300a57621d","collapsed":true,"trusted":true},"cell_type":"code","source":"plot_decision_tree(dt_leaf_model, dt_col_cycle1_0)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9abac991-5b33-41b5-8dcc-9a748d806344","_uuid":"23e035cbf2800e9f602be501759f7d7a1537ab00"},"cell_type":"markdown","source":"That was it for the second cycle. <br>\nIt seems that we had a good assumption by trying different hyperparameters values to obtained a better model","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ef9c090b-f1da-4594-a496-72ffe75b500e","_uuid":"044269ee39f9e58853e40aaab1bc03f9befaeb8b","collapsed":true},"cell_type":"markdown","source":"# Cycle 3","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ec119c99-fe34-483e-96dd-dc646ea4ebad","_uuid":"681edb3095bd5fb35dc71c505165191b7f4e1bd9","collapsed":true},"cell_type":"markdown","source":"## 3.1 Ideas, assumptions\nUntil now we used a limited set of features : Pclass, Fare and Sex. <br> \nLet's see if we can improve the accuracy if we add more features to the training process.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4814d418-de6a-40aa-9a54-7152bff4e047","_uuid":"b5361297d0314f87fcc29c5d601568e30e0d6c16","collapsed":true},"cell_type":"markdown","source":"## 3.2 Implement","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a7726b2f-4162-4bf4-9345-a8d92e66fbaf","_uuid":"2dea97586bef21da65a273906fa90496ea09e77a","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"# Stats about all possible features from training set\ntrain_titanic.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"42ca662d-da6e-4b6e-9225-35f4a83d3a73","_uuid":"bce5eb09dd44ae15efc2865fc6f7b4543f3861b7"},"cell_type":"markdown","source":"Let's investigate Age feature","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ec6b0a48-22b2-47d0-b3c7-212547eb97ba","_uuid":"b196515c7e2ada2506429a39356829d588371be0","collapsed":true,"trusted":true},"cell_type":"code","source":"train_titanic.groupby([\"Age\"])[\"Alive\", \"Not_alive\"]. \\\n    sum(). \\\n    plot.bar(title=\"Age vs survived histogram\", figsize=(20,5))\n    \ntrain_titanic[train_titanic[\"Sex\"] == \"female\"]. \\\n    groupby([\"Age\"])[\"Alive\", \"Not_alive\"]. \\\n    sum(). \\\n    plot.bar(title = \"Female age vs survived histogram\", figsize=(20,5))\n    \ntrain_titanic[train_titanic[\"Sex\"] == \"male\"]. \\\n    groupby([\"Age\"])[\"Alive\", \"Not_alive\"]. \\\n    sum(). \\\n    plot.bar(title = \"Male age vs survived histogram\", figsize=(20,5))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8fc3efb-417b-46ad-a297-352860d4f905","_uuid":"7cb4ea4b6e82c4585e63437062ba8bc7b80b5e5d"},"cell_type":"markdown","source":"What is clean from the above plots is that female had a bigger chance to survived than male, but we already know that. <br>\nThere is not a clear pattern between age and survive. Let's hope that the ML model will find better correlation with other set of features. <br> <br>\nBecause Age column has missing values let's fill them with median, mean or most frequent age value.\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"16e4295e-ec2e-4ea2-a9ac-38208ba923fd","_uuid":"ac9528e2733e0f5ec981837a0331c91760ecaf76","collapsed":true,"trusted":true},"cell_type":"code","source":"feature_imputer_titanic(\"Age\", \"Age_median\", strategy=\"median\")\nfeature_imputer_titanic(\"Age\", \"Age_mean\", strategy=\"mean\")\nfeature_imputer_titanic(\"Age\", \"Age_most_frequent\", strategy=\"most_frequent\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1b19e24-ef4d-4520-a099-2c9a95b118d1","_uuid":"12e237215fc44fa8d0dedf8398e4930a5f6c1970"},"cell_type":"markdown","source":"Let's see how our new ages vs survived histogram looks after we fill the missing values.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"93636f59-4f3b-4aa6-bcc8-b0bb9e4a96cb","_uuid":"462226f50a8d885fddd60a9b5f6725b815e6a6c7","collapsed":true,"trusted":true},"cell_type":"code","source":"train_titanic.groupby([\"Age_median\"])[\"Alive\", \"Not_alive\"]. \\\n    sum(). \\\n    plot.bar(title=\"Age_median vs survived histogram\", figsize=(20,5))\n    \ntrain_titanic.groupby([\"Age_mean\"])[\"Alive\", \"Not_alive\"]. \\\n    sum(). \\\n    plot.bar(title=\"Age_mean vs survived histogram\", figsize=(20,5))\n    \ntrain_titanic.groupby([\"Age_most_frequent\"])[\"Alive\", \"Not_alive\"]. \\\n    sum(). \\\n    plot.bar(title=\"Age_most_frequent vs survived histogram\", figsize=(20,5))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8c3f1ef7-7ab6-4dcd-8432-1d42233509b4","_uuid":"25cd16cf7685e407687efe454b2da5a117208d08"},"cell_type":"markdown","source":"A simplest and recommended way is to fill the missing values with the median. Let's see which strategy get the best score.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"dd4e72a3-6f24-4283-b653-f72bc9c6ab51","_uuid":"3d9cd464f73271e6ac417068cb8a29fa36733356","collapsed":true,"trusted":true},"cell_type":"code","source":"dt_col_cycle3_0 = ['Pclass', 'Sex_encoded', 'Fare_median', 'Age_mean']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2e7ffc28-1fc6-44f4-b1b6-18cc1d30f1a9","_uuid":"10f4e102c00539902f38442df0436ee6d788f19b","collapsed":true,"trusted":true},"cell_type":"code","source":"dt_features_model = DecisionTreeClassifier(random_state=1987, min_samples_leaf=10)\ndt_features_model.fit(train_titanic[dt_col_cycle3_0], train_titanic[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"57457f13-952a-4f02-9761-406d2f5b2acb","_uuid":"8f2b1b98d529e76666d381ca29f4904331fcc02a","collapsed":true,"trusted":true},"cell_type":"code","source":"save_submition_file(\"dt_age_mean_feature_submission.csv\", \\\n                    test_titanic[\"PassengerId\"], dt_features_model.predict(test_titanic[dt_col_cycle3_0]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc90ebc9-5976-4a8a-ba96-2baf5f99d41e","_uuid":"46e0ca834ed45cb357c423464f5827f74f59246f"},"cell_type":"markdown","source":"Bellow are the scores I have obtained using mean, median and most frequent imputer strategy to fill the age missing values. <br>\nSurprising, the best score was using the mean <br>\nAge mean = 0.76076 <br>\nAge median = 0.74641 <br>\nAge most frequent = 0.74641 <br>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"65af8a90-1728-435d-afdd-ca64443bf729","_uuid":"3006cd9fc213928ab0bf7e43ac3d424202878da4","collapsed":true,"trusted":true},"cell_type":"code","source":"display_feature_importance(dt_features_model, dt_col_cycle3_0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"45761b4f-bd5b-4454-a5c5-de5740f5a512","_uuid":"c7da7905d9e4fb199ee06852a506e270efa5cb74"},"cell_type":"markdown","source":"Above we calculated the global mean age and used it to fill the missing values. Maybe a better idea is to calculate the mean age based on groups of people. Bellow I will calculate the mean age for people from the same Pclass group and use this value to fill the missing values. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"bf97b755-a344-48d2-988b-fbfd915aedd3","_uuid":"16862e11d6ab70a29dc40ba1f1d133fdc9139c4d","collapsed":true,"trusted":true},"cell_type":"code","source":"train_titanic[\"Age_mean_window\"] = train_titanic[[\"Pclass\", \"Age\"]] \\\n    .groupby([\"Pclass\"]) \\\n    .apply(lambda x : x.assign(Age_mean_window = lambda x : x.Age.mean())) \\\n    .reset_index(drop=True) \\\n    .apply(lambda row : row[\"Age_mean_window\"] if math.isnan(row[\"Age\"]) else row[\"Age\"] , axis=1)\n \ntest_titanic[\"Age_mean_window\"] = test_titanic[[\"Pclass\", \"Age\"]] \\\n    .groupby([\"Pclass\"]) \\\n    .apply(lambda x : x.assign(Age_mean_window = lambda x : x.Age.mean())) \\\n    .reset_index(drop=True) \\\n    .apply(lambda row : row[\"Age_mean_window\"] if math.isnan(row[\"Age\"]) else row[\"Age\"] , axis=1)\n ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"28678278-79a0-4235-939d-87af4c42958d","_uuid":"59c61ef0751118c06edad4798aab6bfe65503c4b","collapsed":true,"trusted":true},"cell_type":"code","source":"train_titanic.groupby([\"Age_mean_window\"])[\"Alive\", \"Not_alive\"]. \\\n    sum(). \\\n    plot.bar(title=\"Age_mean_window vs survived histogram\", figsize=(20,5))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f923cfff-d3fa-4528-ab6c-52b630501af8","_uuid":"3bff9c8f10912764dc20977967bfda4bcec4156d","collapsed":true,"trusted":true},"cell_type":"code","source":"dt_col_cycle3_1 = ['Pclass', 'Sex_encoded', 'Fare_median', 'Age_mean_window']\ndt_features_model = DecisionTreeClassifier(random_state=1987, max_depth = 5, min_samples_leaf=20)\ndt_features_model.fit(train_titanic[dt_col_cycle3_1], train_titanic[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f58a3fbd-abb9-4921-ba1a-5a6912123b0a","_uuid":"d190d975734068dfd2cc18019d6307254a91d970","collapsed":true,"trusted":true},"cell_type":"code","source":"save_submition_file(\"dt_age_mean_window_submission_0.csv\", \\\n                    test_titanic[\"PassengerId\"], dt_features_model.predict(test_titanic[dt_col_cycle3_1]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"02bad592-849a-4666-9b6e-ccc1ef292910","_uuid":"e38639f469b93a4fb34eb8d5d71ca2ac8eccd824","collapsed":true},"cell_type":"markdown","source":"After using mean age based on Pclass groups and trying many hyperparameters combinations, the best score obtained was 0.79904. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"625c5817-c7dc-4f72-ae97-e17a32cead09","_uuid":"5d9d72634206432aecad4cfc4152124059067183","collapsed":true,"trusted":true},"cell_type":"code","source":"display_feature_importance(dt_features_model, dt_col_cycle3_1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cffdc0bd-411e-48c9-a9d0-ef84179ca17c","_uuid":"4bbc13fe70c1353a6d942a6efd8bb44837ec6df1","collapsed":true,"trusted":true},"cell_type":"code","source":"resume_wrong_predictions(train_titanic, dt_features_model.predict(train_titanic[dt_col_cycle3_1]), groupby_col=[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1d6bbe3c-3107-4d73-8eda-15d276d04991","_uuid":"cc76acef6de2c0c495b9e7ba60526805f6c2bd30"},"cell_type":"markdown","source":"Maybe majority of us would think that the Name column is not relevant for a new feature. I was also one of these. <br>\nReading more kernels about titanic, I found out that in the Name column is specified the title of the person (Mr, Miss, Master, etc) and this information helped to improve the score.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e82069a0-4f24-4868-85ea-5ede212e8fbd","_uuid":"a5384366ffc0ba225f23e9fc9b4a76b4aabd308e","collapsed":true,"trusted":true},"cell_type":"code","source":"train_titanic.Name[:5]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"285f30a2-79a4-4557-88be-bcd83be5ba05","_uuid":"74d63dfaff4f59333490208706fc401593509c17","collapsed":true,"trusted":true},"cell_type":"code","source":"train_titanic[\"Title\"] = train_titanic.apply(lambda row : (row.Name.split(\",\")[1].split(\" \")[1]), axis=1)\ntest_titanic[\"Title\"] = test_titanic.apply(lambda row : (row.Name.split(\",\")[1].split(\" \")[1]), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7ccb3e16-04e0-475b-bf6d-310d97037c30","_uuid":"dac6c6b942d8e43c565f17775064da5597cabc65","collapsed":true,"trusted":true},"cell_type":"code","source":"train_titanic.groupby([\"Title\"])[[\"Title\"]].count()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4cdc5fba-32d3-4c18-bb68-1bc19230bba3","_uuid":"3ad41a35d58560d00d9c95edde420deb17f4bd51","collapsed":true,"trusted":true},"cell_type":"code","source":"# Encode the Title values\ntitleEncoding = LabelEncoder().fit(train_titanic[\"Title\"].append(test_titanic[\"Title\"]))\ntrain_titanic[\"Title_encoded\"] = titleEncoding.transform(train_titanic[\"Title\"])\ntest_titanic[\"Title_encoded\"] = titleEncoding.transform(test_titanic[\"Title\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e6d42567-e38f-41ff-9b36-58933865432b","_uuid":"7416d93019159e85c03e8c2edc18127f38a3917b","collapsed":true,"trusted":true},"cell_type":"code","source":"dt_col_cycle3_3 = ['Pclass', 'Sex_encoded', 'Fare_median', 'Age_mean_window', 'SibSp', 'Parch', \"Title_encoded\"]\ndt_features_model = DecisionTreeClassifier(criterion=\"entropy\", random_state=1987, max_depth = 5, min_samples_leaf=20)\n# dt_features_model = DecisionTreeClassifier(criterion=\"entropy\", min_impurity_decrease = 0.009, random_state=1987)\ndt_features_model.fit(train_titanic[dt_col_cycle3_3], train_titanic[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"11411f85-67a4-49c1-a6b6-9756400fe9a8","_uuid":"5a594d314058bfd4109b523c7dcbe73e74b7d2bb","collapsed":true,"trusted":true},"cell_type":"code","source":"save_submition_file(\"dt_more_features_submission_0.csv\", \\\n                    test_titanic[\"PassengerId\"], dt_features_model.predict(test_titanic[dt_col_cycle3_3]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"000f9134-29c6-4b13-a06d-8670b6f2dc12","_uuid":"5904acc55232a6514781c633de93e756fab4298a"},"cell_type":"markdown","source":"The score is 0.81339. Wow !!! <br>\nAt the moment of writing this kernel, with this score we are on Top 6! <br>\nMy place on the leaderboard is 497/9547 even if the person from 333-th place has the same score ;)","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e357c092-89b0-48c0-80a8-76dc459b583a","_uuid":"4c95438f061d68f30cff4cf392367a3d6f850e68"},"cell_type":"markdown","source":"## 3.3 Evaluate","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"bb2398e3-d454-44d2-ae45-a2d7504ddfc4","_uuid":"646f85927dea07a9cd93476dd1f13e386372cd07","collapsed":true,"trusted":true},"cell_type":"code","source":"# See where the model makes most wrong predictions on training set.\nresume_wrong_predictions(train_titanic, dt_features_model.predict(train_titanic[dt_col_cycle3_3]), groupby_col=[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"281aa0fa-5e09-4fb9-bb6e-ee700994f9e8","_uuid":"c109f8b0f2b4c49a7778bb811c7d72b640a5c6ab","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"resume_wrong_predictions(train_titanic, dt_features_model.predict(train_titanic[dt_col_cycle3_3]), groupby_col=[\"Sex\", \"Pclass\", \"Age_mean_window\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"330bb45b-088c-4e5b-977a-d45169d47e80","_uuid":"144da34cf1d02589507adbcae7a665d91d5d91cf","collapsed":true,"trusted":true},"cell_type":"code","source":"display_feature_importance(dt_features_model, dt_col_cycle3_3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6077c734-d0da-44f8-ba9d-fada5f8fda84","_uuid":"f158abde9046f61cb0a064400c8e54a0325a0a50","collapsed":true,"trusted":true},"cell_type":"code","source":"# This is how the best decision tree looks.\nplot_decision_tree(dt_features_model, dt_col_cycle3_3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"63fe5fed-bc52-43a3-a01f-a6c302eb8aed","_uuid":"e97d1cf5cbed6656622224ad32cf64264accbdbb"},"cell_type":"markdown","source":"I hope you enjoyed reading this kernel ! <br>\nIt's awesome that using one of the simplest machine learning alg. we succedded to be in top 6 on Titanic leaderboard. <br>\nIf you have any others sugestions for other cycles, I would be happy if you would leave a comment !\n","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"b4fbf126200168e58a26e8c9849b104659ac86d1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}