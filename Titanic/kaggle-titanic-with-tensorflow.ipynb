{"cells":[{"metadata":{"_cell_guid":"5110b336-7fee-4ac8-840c-9733362c5cf2","_uuid":"683a495ed02f010ee04c908a9b65163dc6213999"},"cell_type":"markdown","source":"# Disclaimer:\nFor the Titanic dataset, the best performance will likely be achieved by a non-ANN model. But as a student interested in applying artificial neural networks, I thought it'd be a fun/educational challenge! I'm still very new to machine learning and neural networks so feedback is much appreciated!"},{"metadata":{"_cell_guid":"168ed2be-9c48-4627-be72-19cc7f38ddb1","_uuid":"1acb6d833521394738055c504c79c6fd676592d4"},"cell_type":"markdown","source":"# 0) Libraries"},{"metadata":{"_cell_guid":"a3ff0c25-b53a-4b6e-822f-66b11cba5023","_uuid":"ca5b782a1f1724961b7c03f336cb6eaa77416f03","trusted":true},"cell_type":"code","source":"# General Python Utils\nfrom collections import Counter\nimport gc\ngc.enable()\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Scikit-learn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\n\n# Tensorflow\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\ntf.logging.set_verbosity(tf.logging.ERROR)","execution_count":50,"outputs":[]},{"metadata":{"_cell_guid":"34c8a0e1-2b1f-4a06-a6d7-ba4fb255b7c2","_uuid":"4ca6f580a7007fc66b78819642477071fadbb2ed"},"cell_type":"markdown","source":"# 1) Loading in and checking data"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","collapsed":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Okay, let's load in our datasets!\nraw_train_df = pd.read_csv(\"../input/train.csv\")\nraw_test_df = pd.read_csv(\"../input/test.csv\")\nexample_submission_df = pd.read_csv(\"../input/gender_submission.csv\")\n\ntrain_df = raw_train_df.copy(deep=True)\ntest_df = raw_test_df.copy(deep=True)\ntrain_test_lst = [train_df, test_df]","execution_count":51,"outputs":[]},{"metadata":{"_cell_guid":"fdb8d563-e9e9-4fc0-a005-4a09e930db8b","_uuid":"3bf3ebeb33855b1b1eaed1a5e9767784c3aa5804"},"cell_type":"markdown","source":"### First, Let's take a look at the train and test data to make sure everything was loaded okay"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Taking a look at the first few values in the dataframe\ndisplay(train_df.head())\n# Taking a look at the summary statistics for each feature\ndisplay(train_df.describe())","execution_count":52,"outputs":[]},{"metadata":{"_cell_guid":"02704733-8343-4e4c-8334-99e95f2f79a6","_uuid":"aa019690fce8d137c74268edd04e1332ff0f5ae6"},"cell_type":"markdown","source":"The minimum fare of 0.00 stuck out to me as a little odd so I decided to take a little deeper look.\nSeems like maybe crew members or staff working on the Titanic? That said, there is a Jonkheer in the list too so there might have been some free tickets involved for special personages..."},{"metadata":{"_cell_guid":"13b649e6-b944-4cbb-b981-a539ca49f9cb","_uuid":"18d000cde44e0c61b7cd00c7a9b60dea44712ca7","trusted":true},"cell_type":"code","source":"train_df[train_df['Fare'] == 0]","execution_count":53,"outputs":[]},{"metadata":{"_cell_guid":"be8ec174-7444-478f-bd58-85eabeeea407","_uuid":"97a4a39bde68baa8d3e3ed61378e799784cfa8f4","trusted":true},"cell_type":"code","source":"display(test_df.head())\ndisplay(test_df.describe())","execution_count":54,"outputs":[]},{"metadata":{"_cell_guid":"9eff2868-c9c6-4a93-97f7-f45611209f56","_uuid":"beddf0ab0b4752b8076a3f820a71bcdfd40d2b70"},"cell_type":"markdown","source":"# 2) Data preprocessing"},{"metadata":{"_cell_guid":"94be94ec-1cdc-4c82-a76d-7873194bcd4c","_uuid":"d8b211bb26590a8e4e41bcdcb2689dec6516e05a"},"cell_type":"markdown","source":"### Both train and test datasets appear to have NaN values this could cause problems for our model, so let's look at what is missing and how much"},{"metadata":{"_cell_guid":"28ef30d0-441b-4f4c-8500-88ef3f7d6324","_uuid":"52b25321d1d86f979f536a40f018b86a5009293b","trusted":true},"cell_type":"code","source":"display(train_df.isnull().sum())\nprint(\"Total individuals in train set is: {}\".format(len(train_df)))","execution_count":55,"outputs":[]},{"metadata":{"_cell_guid":"a9b3b228-5dc4-4ca1-8db0-5bf137f02882","_uuid":"a2b7f6db26558adcb114011cd54dd73bc30e34e4","trusted":true},"cell_type":"code","source":"display(test_df.isnull().sum())\nprint(\"Total individuals in test set is: {}\".format(len(test_df)))","execution_count":56,"outputs":[]},{"metadata":{"_cell_guid":"d5d41e4f-ab62-478e-8964-8c95d3880afa","_uuid":"704fbbfb89d29e5f34f8d7929e9d778b6f0075f4"},"cell_type":"markdown","source":"### The huge amount of missing Cabin data is worrying, but let's see if it has any predictive power before figuring out what to do"},{"metadata":{"_cell_guid":"eb43abe3-9294-4533-b81d-c1ce2f976923","_uuid":"9234e2cd553001288dfc63de21741c8322bc0f79","trusted":true},"cell_type":"code","source":"# Let's only consider data that has non-NaN Cabin values (Age or Embarked can still be NaN!)\ncabin_df = train_df[train_df['Cabin'].notnull()]\n\n# Let's create a new feature 'deck_level' that groups passengers by deck levels\ncabin_df = cabin_df.assign(deck_level=pd.Series([entry[:1] for entry in cabin_df['Cabin']]).values)\ndisplay(cabin_df.head())\n\nprint(\"Survival chances based on deck level:\")\ncabin_df.groupby(['deck_level'])['Survived'].mean()\n","execution_count":57,"outputs":[]},{"metadata":{"_cell_guid":"2d7d4f35-da01-49e0-a61e-3f542325ea54","collapsed":true,"_uuid":"2a514ed076f9e0d7f0f83ad9b7164be5ddd6a544"},"cell_type":"markdown","source":"So it looks like deck level may be a useful feature to learn. The NaNs are troubling though, we can get around them (hopefully) by adding a new option for the deck_level to be 'U' (for unknown).\n\nLater, we'll use one hot encoding on deck_level before sending it to our neural network."},{"metadata":{"_cell_guid":"66d049f9-fd0e-4475-8f67-b1757f7514de","_uuid":"6809667c833cc9891a1029b62b759b8a881d5c9b","trusted":true},"cell_type":"code","source":"def process_deck_level(train_test_lst):\n    new = []\n    for dataset in train_test_lst:\n        dataset = dataset.copy(deep=True)\n        # Take the first letter of the Cabin entry if it's not nan. Otherwise, it should be labelled as 'U'.\n        dataset = dataset.assign(deck_level=pd.Series([entry[:1] if not pd.isnull(entry) else 'U' for entry in dataset['Cabin']]))\n        # Okay, now let's drop the Cabin column from our dataset\n        dataset = dataset.drop(['Cabin'], axis = 1)\n        new.append(dataset)\n    return (new)\n\ntrain_df, test_df = process_deck_level(train_test_lst)\n\n# Let's check that we did the right thing...\ndisplay(train_df.head())\ndisplay(test_df.head())\n# Let's also recheck what's still missing\ndisplay(train_df.isnull().sum())\ndisplay(test_df.isnull().sum())","execution_count":58,"outputs":[]},{"metadata":{"_cell_guid":"a659f896-2489-4bb0-a833-859f622b0555","_uuid":"358a6ee6a5767991c13cba6da5cc87576bb76c4e"},"cell_type":"markdown","source":"Okay looking better already! "},{"metadata":{"_cell_guid":"6897be95-8665-46e5-8cc2-700a7c95d29b","_uuid":"490b18db30d9de1c29173a6c0c09a12b5249478d"},"cell_type":"markdown","source":"### Update: I ended up not using deck_level as as feature. \n\nMy reasoning is that Pclass already does a better job of helping determine who survives and deck information ends up grouping individuals of different Pclasses together, which would send a mixed signal to the model."},{"metadata":{"_cell_guid":"6c43fc88-89fa-4d54-853f-517526984d23","_uuid":"4086fcbae85780bb05d7178b506ea5671cae6159","trusted":true},"cell_type":"code","source":"train_df.groupby(['Pclass', 'deck_level']).size().unstack(0).plot.bar(stacked=True)\nplt.title(\"Histogram of deck_level grouped by Pclass\")\n_ = plt.ylabel(\"Frequency\")","execution_count":59,"outputs":[]},{"metadata":{"_cell_guid":"0a69337c-cd89-42a7-82bc-06ca9d3557b8","_uuid":"9c8407acaadae17713ca2be69d69fe10e478acba"},"cell_type":"markdown","source":"### Now let's try to address the missing embarked data! First off what are the possible values of embarked?"},{"metadata":{"_cell_guid":"e9c5912e-7316-4107-af2e-e37568c8c227","_uuid":"b038008cb32757336edd88c7878022729b9bb57a","trusted":true},"cell_type":"code","source":"display(set(train_df['Embarked']))\nprint(\"Survival chances based on embarcation:\")\ntrain_df.groupby(['Embarked'])['Survived'].mean()","execution_count":60,"outputs":[]},{"metadata":{"_cell_guid":"23747cf8-4725-460a-bfa6-76b67e876db0","_uuid":"860cd2d6b85aab8698baf950dd861aed3c67ec7b"},"cell_type":"markdown","source":"It looks like people who embarked from Q had a low survival rate and S had an especially low survival rate...\n\nFor this feature, we'll also fill NaN values with 'N' for 'Not known' since filling with C/Q/S looks like it would make a big difference."},{"metadata":{"_cell_guid":"ccd43ffb-0b2e-4825-9d9b-ff85632ce8fc","_uuid":"6f0d9d39e7d8fb1ebb413617ba2a4c1f89f76d10","trusted":true},"cell_type":"code","source":"# Replace NaN values in the 'Embarked' column with 'N'\ntrain_df[['Embarked']] = train_df[['Embarked']].fillna('N')\n# Let's check that we filled things correctly!\ndisplay(set(train_df['Embarked']))\ndisplay(train_df.isnull().sum())","execution_count":61,"outputs":[]},{"metadata":{"_cell_guid":"c201a6bc-7841-4d30-8c42-b94082b84ba4","_uuid":"7594f5ea64c08cf2f822344fa729c99212686452"},"cell_type":"markdown","source":"### Let's take a quick look at the test data and see what to do about the one fare datapoint that is missing"},{"metadata":{"_cell_guid":"5cb23e56-db0e-43c1-a860-3a90fc0767fe","_uuid":"e483c0720342672d0cd426c697f434bd0ee25d82","trusted":true},"cell_type":"code","source":"test_df[test_df['Fare'].isnull()]","execution_count":62,"outputs":[]},{"metadata":{"_cell_guid":"34ae276a-1fc6-4329-90bd-c4e26ad6d9c1","_uuid":"2a5411674a8eb7627242eb314f1af508eb8197cf"},"cell_type":"markdown","source":"### Let's use Pclass to fill our missing value!"},{"metadata":{"_cell_guid":"11104976-7318-43cc-a8f0-7f8f7e11acca","_uuid":"1f0c619aac8fe8fdfd76bbad529d107f0f2419c2","trusted":true},"cell_type":"code","source":"Pclass_Fare_grouping = test_df.groupby([\"Pclass\"])['Fare']\ntrain_df.groupby(['Pclass', pd.cut(train_df['Fare'], np.arange(0, 701, 5))]).size().unstack(0).plot.bar(stacked=True, title = 'Fare histogram grouped by Pclass')\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nprint(\"Mean Fare for each Pclass:\")\ndisplay(Pclass_Fare_grouping.mean())\nprint(\"Median Fare for each Pclass:\")\ndisplay(Pclass_Fare_grouping.median())","execution_count":63,"outputs":[]},{"metadata":{"_cell_guid":"918a5990-8221-4098-96dd-0b330b24f1a6","_uuid":"66e280ee2d0d97f0261aa595536017a7a8e4f708"},"cell_type":"markdown","source":"The tail for the Fare for Pclass 3 is a bit long so it's probably safer to fill with the median value for that Pclass.\n\nAll this work for one missing fare is overkill, but it's a good exercise in thinking about how to impute data!"},{"metadata":{"_cell_guid":"ed79eccf-7260-4515-b458-87e0d6e1e1de","_uuid":"c4a4ad9a07da915927803f6f4c2c2ae421b3c7d8","trusted":true},"cell_type":"code","source":"test_df[['Fare']] = test_df[['Fare']].fillna(Pclass_Fare_grouping.median()[3])\n# Let's check that our one fill worked!\ndisplay(test_df[test_df['PassengerId'] == 1044])\ndisplay(test_df.isnull().sum())","execution_count":64,"outputs":[]},{"metadata":{"_cell_guid":"119247fb-2a6d-4ee3-94b3-aaa8726a5fd4","_uuid":"2402b0e25729397cb13c95d4a96c00157753e9fb"},"cell_type":"markdown","source":"### Now to figure out what to do about missing age data. Let's do a quick analysis of age before imputing any values!"},{"metadata":{"_cell_guid":"f1c763be-ad00-4136-9772-2c30ae841cc3","_uuid":"f38a712ffe126966efda5f28886873b1d5a30701"},"cell_type":"markdown","source":"### Let's first just take a look at the age distribution in our training set."},{"metadata":{"_cell_guid":"7f12e7e8-3a78-4222-b356-c1bcb623fa75","_uuid":"4185d967ab85f11b963df71ea1fec40077fb797e","trusted":true},"cell_type":"code","source":"ax = train_df[['Age']].plot(kind='hist', bins=20)\nplt.xlabel(\"Age\")\n_ = plt.title(\"Age histogram\")","execution_count":65,"outputs":[]},{"metadata":{"_cell_guid":"42109037-eecd-4c67-8652-ae63202b10ee","_uuid":"12794085c4d77a4c9a0ff1623f8dcc1caddbf84b"},"cell_type":"markdown","source":"### Next, let's look at the relationship between Age and survival"},{"metadata":{"_cell_guid":"466affab-9e79-4f0d-aeae-c6b7d5cab7f9","_uuid":"ad0a68381302dfc86f5b2ae8b219676312bdccfb","trusted":true},"cell_type":"code","source":"train_df.groupby(['Survived', pd.cut(train_df['Age'], np.arange(0, 100, 5))]).size().unstack(0).plot.bar(stacked=True, alpha=0.75)\n_ = plt.title(\"Age histogram grouped by survival\")","execution_count":66,"outputs":[]},{"metadata":{"_cell_guid":"364bf38a-1a24-4db8-ae79-9900939018af","_uuid":"d8764cbf1092cc946598c94353cc90016515272c"},"cell_type":"markdown","source":"* So an initial analysis shows that younger passengers ( < 6) were much more likely to survive than not.\n\n* Agewise, the worst outcomes were for folks in their late teens and early 20's. \n\n* Bad outcomes also for people between age ~24 and ~32 as well."},{"metadata":{"_cell_guid":"5b8301ec-60fc-4939-8489-019966e46636","_uuid":"4ed7da5d44bd97082dc1499ff6aad7b50615e15e"},"cell_type":"markdown","source":"### What about the effect of gender and age on survival?"},{"metadata":{"_cell_guid":"b91e5a64-517a-4bcb-9909-b8bbd371b0cb","_uuid":"85cd7bb57d9b3d3ea817cdc2720457a660c783d8","trusted":true},"cell_type":"code","source":"train_df.groupby(['Survived', 'Sex', pd.cut(train_df['Age'], np.arange(0, 100, 5))]).size().unstack(0).plot.bar(stacked=True, alpha=0.75)\nplt.title(\"Age histogram grouped by survival and gender\")\nplt.tight_layout()","execution_count":67,"outputs":[]},{"metadata":{"_cell_guid":"5ee7a0e0-03f4-455b-b647-f268a62e0726","collapsed":true,"_uuid":"b297ee0da51552630c02db017694743558172e97"},"cell_type":"markdown","source":"* This plot is a bit messy, but the left side shows pretty clearly that females had a high survival rate.\n\n* Looking at right side paints the opposite picture (with the exception if you were a male under 6, then youre survival chances were pretty good).\n\n### This quick set of observations seem to suggest that getting age right for young children  is important for predicting survival."},{"metadata":{"_cell_guid":"2ec5a861-7008-439c-b47b-28a58ec6c3a2","_uuid":"1ec6981002eba896200b179d6249d2e1377da44a"},"cell_type":"markdown","source":"### One promising strategy to impute age that seems to work well in other kernels is to use the name title"},{"metadata":{"_cell_guid":"70b995c5-5632-45a6-a51d-0da21d2361c4","_uuid":"4eaf32475c776e01e04154ea214427f87961bb3a","trusted":true},"cell_type":"code","source":"# All name formats seem to be something like:\n# \"last_name, title. first_name \"nickname\" (full_name)\"\n# To get title, we split the string by comma and select the second half. Then we split that second half by '.' and take the first half\n# i.e.\n# 1) [\"last_name\", \"title. first_name \"nickname\" (full_name)\"] (select element 1!)\n# 2) [\"title\", \"first_name \"nickname\" (full_name)\"] (select element 0!)\ntrain_titles = [name.split(',')[1].lstrip(' ').split('.')[0] for name in train_df['Name']]\n# Let's see if the above strategy works\nprint(\"Train set titles (and counts):\")\nprint(Counter(train_titles))\n\nprint(\"\\nTest set titles (and counts):\")\ntest_titles = [name.split(',')[1].lstrip(' ').split('.')[0] for name in test_df['Name']]\nprint(Counter(test_titles))\n\nprint(\"\\n===============================\")\n\nage_missing_train_titles = [name.split(',')[1].lstrip(' ').split('.')[0] for name in train_df[train_df['Age'].isnull()]['Name']]\nprint(\"\\nTrain set titles (and counts) with missing ages:\")\nprint(Counter(age_missing_train_titles))\n\nage_missing_test_titles = [name.split(',')[1].lstrip(' ').split('.')[0] for name in test_df[test_df['Age'].isnull()]['Name']]\nprint(\"\\nTest set titles (and counts) with missing ages:\")\nprint(Counter(age_missing_test_titles))","execution_count":68,"outputs":[]},{"metadata":{"_cell_guid":"97f9ab06-c773-436a-aa97-557403403059","_uuid":"b4d9a62a78f5edcba222181c8cdf1fe5771d2fec"},"cell_type":"markdown","source":"### Looks like we have a nice list of titles, let's add them to our dataframe for now"},{"metadata":{"_cell_guid":"3a17c6ef-bec9-425b-8b89-26f31d0674b4","_uuid":"d459369ccf095c0483ecac18d220c1e921a39dc0","trusted":true},"cell_type":"code","source":"# Let's add the titles as a new feature for our dataset\ndef naive_process_title(train_test_lst):\n    new = []\n    for dataset in train_test_lst:\n        dataset = dataset.copy(deep=True)\n        titles = [name.split(',')[1].lstrip(' ').split('.')[0] for name in dataset['Name']]\n        dataset = dataset.assign(title=pd.Series(titles).values)\n        new.append(dataset)\n    return (new)\n\ntrain_df, test_df = naive_process_title([train_df, test_df])\n\n# Taking a look at our dataframes to make sure we did the right thing...\ndisplay(train_df.head())\ndisplay(test_df.head())","execution_count":69,"outputs":[]},{"metadata":{"_cell_guid":"1c41dc87-cd02-4d8f-9ea6-8436c2f267de","_uuid":"d1b3cfa4fb0fe2bb142d3b0bf44e74dccb69e9ae"},"cell_type":"markdown","source":"### I'm not super well versed with titles from \"back in the day\" so let's see if we can discover how age (the thing we want to impute) relates to title"},{"metadata":{"_cell_guid":"9106df67-d258-478d-aa08-9ca30b98574e","collapsed":true,"_uuid":"09e77725d0e3f9a2ab5a250aa69e846b0336d256","trusted":true},"cell_type":"code","source":"def plot_title_age_hist(title, train_df, bins=20):\n    title_ages = train_df[train_df['title'] == title]['Age']\n    title_ages.plot(kind='hist', bins=bins, legend=True)\n    title_ages.describe()\n    plt.xlabel(\"Age\")\n    plt.title(\"Age histogram for '{}' title\".format(title))","execution_count":70,"outputs":[]},{"metadata":{"_cell_guid":"b4e21465-b79d-4cff-822a-ee9d3ef76820","_uuid":"1ae1b1228c174c869325e604b725d6cfced204dd","trusted":true},"cell_type":"code","source":"title_groups = train_df.groupby(['title'])\ndisplay(title_groups['Age'].describe())\nplot_title_age_hist(\"Master\", train_df, bins=10)","execution_count":71,"outputs":[]},{"metadata":{"_cell_guid":"2c4d39f3-ebca-43b7-b862-7d047c70152b","_uuid":"17c9978d53057cd3cafe7383a3060848a913abe3"},"cell_type":"markdown","source":"* It looks like 'Master' is a reliable signal for young boy."},{"metadata":{"_cell_guid":"1e28f255-2bd7-4c64-8225-7a4dde20a23e","_uuid":"5da28e7be84a3b2a6492f587a779ddae725342de","trusted":true},"cell_type":"code","source":"plot_title_age_hist('Miss', train_df)","execution_count":72,"outputs":[]},{"metadata":{"_cell_guid":"e15c31e4-b68c-4599-8025-d4785a6038b6","_uuid":"57fec7fc70c1445e89fa03ada671646e7130c083"},"cell_type":"markdown","source":"* Miss looks like the corresponding title, but it can take on a much much larger variation of values..."},{"metadata":{"_cell_guid":"462363dd-73a8-4004-b474-0658993493fc","_uuid":"2935a42043a1026bad969717c1739b47e8259bd7"},"cell_type":"markdown","source":"### Let's see if we can get more specific ages to impute for the \"miss\" title by using the 'Parch' feature"},{"metadata":{"_cell_guid":"e0a90256-0012-46e4-a5e7-ad56be0caa44","_uuid":"5a1bfa17122282705d3f003cb3fd68026b3ae007","trusted":true},"cell_type":"code","source":"def title_feature_age_analysis(title, feature, train_df):\n    # Let's loop through all values of our feature of interest (in this case \"Parch\")\n    title_df =train_df[(train_df['title'] == title)]\n    title_df.groupby([feature, pd.cut(title_df['Age'], np.arange(0, 100, 5))]).size().unstack(0).plot.bar(stacked=True, alpha=0.75)\n    plt.xlabel(\"Age\")\n    plt.ylabel(\"Frequency\")\n    _ = plt.title(\"Age histogram for '{}' title grouped by {}\".format(title, feature))\n    for i in range(max(train_df[train_df['title'] == title][feature]) + 1):\n        # Print common descriptive stats for our title and the given level of our feature\n        print(\"Statistics for '{}' title with {} of: {}\".format(title, feature, i))\n        display(train_df[(train_df['title'] == title) & (train_df[feature] == i)]['Age'].describe())\n        print(\"Median\\t{}\\n\".format(train_df[(train_df['title'] == title) & (train_df[feature] == i)]['Age'].median()))\n        print(\"=========================\\n\")\n\ntitle_feature_age_analysis('Miss', 'Parch', train_df)","execution_count":73,"outputs":[]},{"metadata":{"_cell_guid":"f7e73ac1-baa7-408f-a75e-79f115e8e69f","_uuid":"baadd65ce4f2447d216c82b32c2ca07da0262c2a"},"cell_type":"markdown","source":"### Cool! A parch of 1 or 2 together with the 'Miss' title seems to be quite indicative of younger age! Does our finding in the train dataset hold up in the test dataset?"},{"metadata":{"_cell_guid":"bb53b27b-7078-41e8-9fa7-52d7a811db12","_uuid":"ff0d3bd079c941040297d8a135527e10cfecc8d0","trusted":true},"cell_type":"code","source":"title_feature_age_analysis('Miss', 'Parch', test_df)","execution_count":74,"outputs":[]},{"metadata":{"_cell_guid":"f422d24b-7967-414e-a60b-3c014433a04f","_uuid":"bbde2194f4648fcece9452ef5d4b5021992172d5"},"cell_type":"markdown","source":"### Besides 'Miss' and 'Master' we'll also have to fill many more missing ages with 'Mr' and 'Mrs'. Let's see if we can use Parch to help us out again!"},{"metadata":{"_cell_guid":"a10381f4-2160-4fe5-8430-3ba292b2a43b","_uuid":"255fc0aa9007a83baf57476c7a44204e8a4a08a2","trusted":true},"cell_type":"code","source":"title_feature_age_analysis('Mrs', \"Parch\", train_df)","execution_count":75,"outputs":[]},{"metadata":{"_cell_guid":"076c97f4-5005-4ae5-b008-6ac7ace1f547","_uuid":"d487a546fe530f724d8bdad98796e313ae4aa641","trusted":true},"cell_type":"code","source":"title_feature_age_analysis('Mr', \"Parch\", train_df)","execution_count":76,"outputs":[]},{"metadata":{"_cell_guid":"75e048b9-1ab4-41b3-995a-04611659b871","_uuid":"c42f9e64b672141a7bbc505700ec7bd82030a6a3"},"cell_type":"markdown","source":"So it looks like \"Parch\" is not super helpful for narrowing the age of 'Mr' and 'Mrs' titles. Let's try using the median to fill these titles then..."},{"metadata":{"_cell_guid":"f461ac10-29fd-48d5-ba6a-4c58709ff968","_uuid":"81e104df60e30755827bdb2aff6a6f96c47144b7"},"cell_type":"markdown","source":"### ~~We've taken a bit of a look at titles and their relation to age, time to fill in our missings age values with the above information~~\n\n### Update: after a lot of tests, it seems that imputing age seems to hurt the predictive power of the model more than help. So I will opt for a two model strategy where I will train a model that includes 'Age' related features and train another model where all 'Age' related features are removed. That said, the data exploration was still helpful!"},{"metadata":{"_cell_guid":"9035b261-af70-4851-85c0-a3fefb743bb7","collapsed":true,"_uuid":"587d84ec52dc590f7bd21dd060873ae92676e4f0","trusted":true},"cell_type":"code","source":"# Code to fill in missing NaN datapoints based on title analysis. No longer used but it's here for those interested in using it.\n# def age_imputer(train_test_lst):\n#     new = []\n#     for dataset in train_test_lst:\n#         dataset = dataset.copy(deep=True)\n#         # This is the list of unique titles for individuals with a NaN age\n#         missing_age_titles = list(set([name.split(',')[1].lstrip(' ').split('.')[0] for name in dataset[dataset['Age'].isnull()]['Name']]))\n#         print(\"Titles for individuals with missing age are: {}\".format(missing_age_titles))\n#         for title in missing_age_titles:\n#             # Fill in missing ages for 'Mr'/'Mrs'/'Master'/'Ms'/'Dr' titles\n#             if (title in ['Mr', 'Mrs', 'Master', 'Ms', 'Dr']):\n#                 median = dataset[(dataset['title'] == title)]['Age'].median()\n#                 # Treat 'Ms' as 'Mrs'\n#                 if (title == 'Ms'):\n#                     median = dataset[(dataset['title'] == 'Mrs')]['Age'].median()\n#                 dataset[(dataset['title'] == title) & (dataset['Age'].isnull())] = dataset[(dataset['title'] == title) & (dataset['Age'].isnull())].fillna(median)\n#             # Fill in missing ages for \"Miss\" titles\n#             elif (title == 'Miss'):\n#                 for level in range(max(dataset[dataset['title'] == title]['Parch']) + 1):\n#                     df = dataset[(dataset['title'] == 'Miss') & (dataset['Age'].isnull()) & (dataset['Parch'] == level)]\n#                     if (not df.empty):\n#                         median = dataset[(dataset['title'] == title) & (dataset['Parch'] == level)]['Age'].median()\n#                         dataset[(dataset['title'] == 'Miss') & (dataset['Age'].isnull()) & (dataset['Parch'] == level)] = dataset[(dataset['title'] == 'Miss') & (dataset['Age'].isnull()) & (dataset['Parch'] == level)].fillna(median)\n#         new.append(dataset)\n#     return (new)\n\n# train_df, test_df = age_imputer([train_df, test_df])","execution_count":77,"outputs":[]},{"metadata":{"_cell_guid":"3e2ebc27-ff29-4eda-848a-776577adf6b9","collapsed":true,"_uuid":"2b98ba263d29d425ec77c2cb94b8ffe65e2448a9","trusted":true},"cell_type":"code","source":"# display(train_df.isnull().sum())\n# display(test_df.isnull().sum())","execution_count":78,"outputs":[]},{"metadata":{"_cell_guid":"4c2cb38f-ccbe-44d5-9d35-72e5574fc2c4","_uuid":"999b939ac203ce534a85e2e73bafe7fb5775cbd8"},"cell_type":"markdown","source":"### ~~Looks like all the NaNs got filled in. But we should do some sanity checks to verify that things got filled in **correctly**.~~"},{"metadata":{"_cell_guid":"f3b3be94-3292-466d-9808-0f72774c20a5","collapsed":true,"_uuid":"877ed7c8c94e751db888b4f1e057eb93af2a801c","trusted":true},"cell_type":"code","source":"# display(raw_train_df[raw_train_df['Age'].isnull()])","execution_count":79,"outputs":[]},{"metadata":{"_cell_guid":"9059d4e4-451e-499f-87c7-4d2e782c0eb6","collapsed":true,"_uuid":"517601b5b7f6b7384cd6ec2ed0b9bd83b6e0ef40","trusted":true},"cell_type":"code","source":"# # Select passengers that have NaN ages in our raw_train_df\n# train_df.loc[train_df['PassengerId'].isin(raw_train_df[raw_train_df['Age'].isnull()]['PassengerId'])]","execution_count":80,"outputs":[]},{"metadata":{"_cell_guid":"07b89b8e-c850-43fe-8c48-706a2372eb0d","_uuid":"568d7cb8f21259a18b9e4abc4296a36cac63cdce"},"cell_type":"markdown","source":"### ~~How does our new distribution look?~~"},{"metadata":{"_cell_guid":"bbd97e9f-8e7f-4901-b053-3a402d63e811","collapsed":true,"_uuid":"ffe242d8a583304a0e6e65e189edf45829ad6852","trusted":true},"cell_type":"code","source":"# fig, (ax1, ax2) = plt.subplots(1, 2)\n# # First column plot\n# train_df[['Age']].plot(kind='hist', bins=20, ax=ax1, legend=False)\n# ax1.set_xlabel(\"Age\")\n# ax1.set_title(\"NaN filled\")\n# ymin, ymax = ax1.get_ylim()\n# # Second column plot\n# raw_train_df[['Age']].plot(kind='hist', bins=20, ax=ax2, sharey=True, legend=False)\n# ax2.set_ylim(ymin, ymax)\n# ax2.set_xlabel(\"Age\")\n# _ = ax2.set_title(\"Original distribution\")","execution_count":81,"outputs":[]},{"metadata":{"_cell_guid":"3af17f6d-2b11-4451-b2fd-12590ac30716","_uuid":"9cb5075555ab34098ed704a968833b51a684245d"},"cell_type":"markdown","source":"~~Looks like things got filled in properly. We now have a massive peak at 30 and 35 years of age due to the large number of fills we made (for 'Mr' and 'Mrs' titles) but the rest of the distribution looks to be preserved...~~"},{"metadata":{"_cell_guid":"a5b85cca-e4b7-46c9-b165-836dd2a0fc3e","_uuid":"8b58ac1f09802a8ae72fc6275f626ddcb71a1f09"},"cell_type":"markdown","source":"# 3) Some feature engineering (and additional data exploration)"},{"metadata":{"_cell_guid":"bccddb14-2f0e-49a3-a374-fffbfe3d57fd","_uuid":"658caf12f32624face05f060d4bb80ed3c020f19"},"cell_type":"markdown","source":"While going through the data, I happened to stumble on the unfortunate 'Sage' family which had a 0 survival rate despite having women and children (which normally have a high survival rate)."},{"metadata":{"_cell_guid":"59e66b14-ee7b-4846-be77-b1556a409527","_uuid":"dbe89b420535a30b209aedcbc76c9d345b262771","trusted":true},"cell_type":"code","source":"raw_train_df[raw_train_df['Name'].str.startswith('Sage,')]","execution_count":82,"outputs":[]},{"metadata":{"_cell_guid":"021c6973-7eb2-48c2-93a5-56217df41eeb","_uuid":"a0aae01ccb0c8a2ca59267416c60bd669fd839c2"},"cell_type":"markdown","source":"Maybe, family size and/or the Pclass also played a big role in survivorship. Let's engineer some features and explore!"},{"metadata":{"_cell_guid":"dc9b787a-79c6-48d2-ab86-4303d328a4f1","_uuid":"498fa55b51c67b982bc7e5eb0e84f630a077a101","trusted":true},"cell_type":"code","source":"# Let's add family_size which is just the sum of 'SibSp' and 'Parch'\ntrain_df['family_size'] = train_df['SibSp'] + train_df['Parch']\ntest_df['family_size'] = test_df['SibSp'] + test_df['Parch']\n# Check that things were added properly\ndisplay(train_df.head())\n# Plot family size grouped by survival\ntrain_df.groupby(['Survived', pd.cut(train_df['family_size'], np.arange(0, 11))]).size().unstack(0).plot.bar(stacked=True)\nplt.xlabel(\"Family Size\")\n_ = plt.title(\"Histogram of family size grouped by survival\")","execution_count":83,"outputs":[]},{"metadata":{"_cell_guid":"c659d661-cfee-4803-8e75-b8655109e495","_uuid":"e1e60a0820248626059a5751de4f97787907aa7c"},"cell_type":"markdown","source":"So yeah... large family is definitely not good for survival... Let's see if Pclass can help us further predict survival."},{"metadata":{"_cell_guid":"5db08fa5-b549-42e0-b540-856d9f073aaf","_uuid":"ceb6b1bc79912bdea4bc4c702fb7b43f0d565dd3","trusted":true},"cell_type":"code","source":"train_df.groupby(['Survived', 'Pclass', pd.cut(train_df['family_size'], np.arange(0, 11))]).size().unstack(0).plot.bar(stacked=True)\nplt.ylabel(\"Frequency\")\n_ = plt.title(\"Histogram of Pclass x family size grouped by survival\")","execution_count":84,"outputs":[]},{"metadata":{"_cell_guid":"9f59c397-aaa9-4924-b411-d973fb9832e4","_uuid":"8fb8b1660bfb28c450854830fe27cf7db291f0a4","trusted":true},"cell_type":"code","source":"train_df.groupby(['Survived', pd.cut(train_df['Pclass'], np.arange(0, 4))]).size().unstack(0).plot.bar(stacked=True)\n_ = plt.title(\"Histogram of Pclass grouped by survival\")","execution_count":85,"outputs":[]},{"metadata":{"_cell_guid":"f90ae86b-163e-4daf-afc7-d841bcab83d1","_uuid":"e430eb1456c854c84ac861aa9e66c6e017fa321d"},"cell_type":"markdown","source":"Being in Pclass gives much higher chance of survival but let's drill in more and look at gender too"},{"metadata":{"_cell_guid":"d2baff85-d6fa-41ca-a5c8-fefb6472faef","_uuid":"7c417368c155f83bd14ae456f61e30c16147410a","trusted":true},"cell_type":"code","source":"train_df.groupby(['Survived', 'Sex', pd.cut(train_df['Pclass'], np.arange(0, 4))]).size().unstack(0).plot.bar(stacked=True)\n_ = plt.title(\"Histogram of gender x Pclass grouped by survival\")\nplt.tight_layout()","execution_count":86,"outputs":[]},{"metadata":{"_cell_guid":"0659d2b7-da39-448d-bae6-8e166924a0e2","_uuid":"71455e17f8ba03b7428d597266c6bfec5c237a31"},"cell_type":"markdown","source":"Looks like if you were a female in class 1 or 2 your chances were pretty great. Pclass 3 females had more of a 50/50 chance.\n\nAs a male things look much more grim. But Pclass 1 and 2 males still fare better than those in 3.\n\nSince I'm using tensorflow, we can create the 'Pclass' and 'Sex' feature cross in the data pipeline (below)."},{"metadata":{"_cell_guid":"200d9f3d-2936-4c42-86e5-0d3efa6bca2a","_uuid":"b554357ed28e3bc6de3f708c51856d5755b946f4"},"cell_type":"markdown","source":"Let's look at Gender x Age effects on survival"},{"metadata":{"_cell_guid":"03b33084-ade4-4bcd-a2f7-d293240f0623","_uuid":"e9f9fff839c5b04c386c932b71e1b3b7e8237ee2","trusted":true},"cell_type":"code","source":"train_df.groupby(['Survived', 'Sex', pd.cut(train_df['Age'], np.arange(0, 80, 10))]).size().unstack(0).plot.bar(stacked=True)\n_ = plt.title(\"Histogram of gender x Age grouped by survival\")\nplt.tight_layout()","execution_count":87,"outputs":[]},{"metadata":{"_cell_guid":"40cc072c-2766-4132-a5d7-0ecb2b8d0491","_uuid":"badb14a9cf6bbf47819a6f080faae48bc12d6984"},"cell_type":"markdown","source":"But, we can probably do even better! Let's look at Gender x Pclass x Age effects on survival"},{"metadata":{"_cell_guid":"ab45388b-304f-4976-af8e-d092233b40e6","_uuid":"58ed51e4e947e6ee1339fc94b58de905f507410d","trusted":true},"cell_type":"code","source":"train_df.groupby(['Survived', 'Sex', 'Pclass', pd.cut(train_df['Age'], np.arange(0, 80, 10))]).size().unstack(0).plot.bar(stacked=True)\n_ = plt.title(\"Histogram of gender x Pclass x Age grouped by survival\")\nplt.tight_layout()","execution_count":88,"outputs":[]},{"metadata":{"_cell_guid":"57fee955-f887-4500-8ad5-22c6f282bdcd","_uuid":"c41b6c1300a9a9b1c1c193f1de393361a7424ed4"},"cell_type":"markdown","source":"# 4) Assembling pipeline for tensorflow"},{"metadata":{"_cell_guid":"3b6240cc-b5da-4dd0-884b-2a70b91d9e6b","collapsed":true,"_uuid":"6894dd731c77a4bf4dbdf4ac03bab7d42f6f5f48","trusted":true},"cell_type":"code","source":"# To get things to work nicely with tensorflow we'll need to subtract one from 'Pclass' so our classes start at 0\ntrain_df['Pclass'] = train_df['Pclass'] - 1\ntest_df['Pclass'] = test_df['Pclass'] - 1","execution_count":89,"outputs":[]},{"metadata":{"_cell_guid":"603e0553-aa56-42c6-a307-e0f1d8e24c7c","_uuid":"866f278f77e202b5e1749a957745189876668677","trusted":true},"cell_type":"code","source":"# One last check over all our data\ntrain_df","execution_count":90,"outputs":[]},{"metadata":{"_cell_guid":"1b922d83-ce21-4e09-8c3b-05ee655caed7","_uuid":"e22a2f2ce139ad5fa574bb02edc82917410b050e","trusted":true},"cell_type":"code","source":"# Let's remind ourselves of the data columns we have\ntrain_df.columns","execution_count":91,"outputs":[]},{"metadata":{"_cell_guid":"71563361-0423-4379-b07d-3cd79affa3aa","collapsed":true,"_uuid":"b6102ca2584a4e170c76661c7840404e5a29d5ae","trusted":true},"cell_type":"code","source":"def build_feature_columns(use_age=False):\n    \"\"\"\n    Build our tensorflow feature columns!\n    \n    For a great overview of the different feature columns in tensorflow and when to use them, see:\n    https://www.tensorflow.org/versions/master/get_started/feature_columns\n    \n    We'll build a set of wide features (when we need to learn feature interactions) as well as deep features (when we need generalization)\n    \"\"\"\n    # ======== Basic features =========\n    # Categorical features\n    Pclass = tf.feature_column.categorical_column_with_identity(\"Pclass\", num_buckets = 3)\n    Sex = tf.feature_column.categorical_column_with_vocabulary_list(\"Sex\", [\"female\", \"male\"])\n    Embarked = tf.feature_column.categorical_column_with_vocabulary_list(\"Embarked\", [\"C\", \"N\", \"Q\", \"S\"])\n    #Name = tf.feature_column.categorical_column_with_hash_bucket(\"Name\", hash_bucket_size = 100)\n    Ticket = tf.feature_column.categorical_column_with_hash_bucket(\"Ticket\", hash_bucket_size = 400)\n    \n    # Continuous features\n    SibSp = tf.feature_column.numeric_column(\"SibSp\")\n    Parch = tf.feature_column.numeric_column(\"Parch\")\n    Fare = tf.feature_column.numeric_column(\"Fare\")\n    if (use_age):\n        Age = tf.feature_column.numeric_column(\"Age\")\n    \n    # ======== Engineered features =======\n    # Basic engineered features\n    #deck_level = tf.feature_column.categorical_column_with_vocabulary_list(\"deck_level\", [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"T\", \"U\"])\n    family_size = tf.feature_column.numeric_column(\"family_size\")\n    title = tf.feature_column.categorical_column_with_hash_bucket(\"title\", hash_bucket_size = 10)\n    \n    # Bucketed features\n    fare_buckets = tf.feature_column.bucketized_column(Fare, boundaries=list(range(0, 200, 5)))\n    family_size_buckets = tf.feature_column.bucketized_column(family_size, boundaries=[1, 2, 3, 4, 5, 6, 7])\n    Parch_buckets = tf.feature_column.bucketized_column(Parch, boundaries=[1,2,3,4,5,6])\n    SibSp_buckets = tf.feature_column.bucketized_column(SibSp, boundaries=[1,2,3,4,5,6])\n    \n    # Crossed features\n    Pclass_x_Sex = tf.feature_column.crossed_column(keys = [Pclass, Sex], hash_bucket_size = 10)\n    Pclass_x_family_size = tf.feature_column.crossed_column(keys = [Pclass, family_size_buckets], hash_bucket_size = 30)\n    Pclass_x_Sex_x_Embarked = tf.feature_column.crossed_column(keys = [Pclass, Sex, Embarked], hash_bucket_size = 30)\n    \n    # Age features\n    if (use_age):\n        age_buckets = tf.feature_column.bucketized_column(Age, boundaries=[5, 15, 25, 35, 45, 55, 65])\n        Pclass_x_Sex_x_age_buckets = tf.feature_column.crossed_column(keys = [Pclass, Sex, age_buckets], hash_bucket_size = 100)\n    \n    # =========== Putting together wide features =================\n    wide_features = set([Pclass, Sex, Ticket, Embarked, title, fare_buckets, Parch_buckets, SibSp_buckets, family_size_buckets,\n                         Pclass_x_Sex, Pclass_x_family_size, Pclass_x_Sex_x_Embarked])\n    if (use_age):\n        wide_features |= set([age_buckets, Pclass_x_Sex_x_age_buckets])\n    \n    # =========== Putting together deep features =================\n    deep_features = set([SibSp, Parch, Fare, family_size,\n                         tf.feature_column.indicator_column(Pclass),\n                         tf.feature_column.indicator_column(Sex),\n                         tf.feature_column.indicator_column(title),\n                         tf.feature_column.indicator_column(Embarked),\n                         tf.feature_column.embedding_column(Ticket, dimension = 10)])\n    if (use_age):\n        deep_features |= set([Age])\n\n    return((wide_features, deep_features))","execution_count":92,"outputs":[]},{"metadata":{"_cell_guid":"bfeec7a6-9efa-458f-938e-b02c8022151e","collapsed":true,"_uuid":"27eec935f2eb1c9041f0e431c19140ae9ba5ef3e","trusted":true},"cell_type":"code","source":"def input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    \"\"\"\n    This is our input function that will pass data into the tensorflow DNN class we'll create.\n    It takes in a pandas dataframe.\n    It outputs a tensorflow dataset one_shot_iterator\n    \"\"\"\n    # Convert pandas df to dict of numpy arrays\n    features = {key:np.array(value) for key, value in dict(features).items()}\n    # Put together the tensorflow dataset. Configures batching/repeating.\n    dataset = Dataset.from_tensor_slices((features, targets))\n    dataset = dataset.batch(batch_size).repeat(num_epochs)\n    # Shuffle data\n    if (shuffle):\n        dataset = dataset.shuffle(buffer_size = 50000)\n    features, labels = dataset.make_one_shot_iterator().get_next()\n    return (features, labels)","execution_count":93,"outputs":[]},{"metadata":{"_cell_guid":"3f893f2b-4fab-4687-aebf-f503045739d8","collapsed":true,"_uuid":"541639e5f74de68f379a160c8c63c9d0095f6f5c","trusted":true},"cell_type":"code","source":"## ============= Previous train_test_split that was used ===========\n# train_ex_df = train_df.sample(frac=0.60)\n# train_targ_series = train_ex_df['Survived']\n\n# xval_ex_df = train_df.drop(train_ex_df.index)\n# xval_targ_series = xval_ex_df['Survived']\n\n# # Double check that we don't have any train_ex_df data in our xval_ex_df data\n# assert(not any(train_ex_df[\"PassengerId\"].isin(xval_ex_df[\"PassengerId\"])))\n\n# # Select our variables of interest\n# train_ex_df = train_ex_df[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"family_size\"]]\n# xval_ex_df = xval_ex_df[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"family_size\"]]\n\n# print(\"Total training samples: {}\".format(len(train_df)))\n# print(\"New training split: {}\".format(len(train_ex_df)))\n# print(\"New xval split: {}\".format(len(xval_ex_df)))","execution_count":94,"outputs":[]},{"metadata":{"_cell_guid":"b6a25d18-4404-4dd9-a44c-db6488506876","_uuid":"3af244b89788b3f7831be2be4d5e9f7a481baa9a"},"cell_type":"markdown","source":"# 5) Building our classifier model with tensorflow"},{"metadata":{"_cell_guid":"ad70ab20-dd2d-4773-8992-b2678ca42130","collapsed":true,"_uuid":"08e7ce8313a098968489d1ff6c737f95812372b2","trusted":true},"cell_type":"code","source":"def plot_acc(train_accs, val_accs):\n    fig, ax = plt.subplots(1, 1)\n    ax.set_ylabel(\"Accuracy\")\n    ax.set_xlabel(\"Period\")\n    ax.set_title(\"DNN model accuracy vs. Period\")\n    ax.plot(train_accs, label = \"train\")\n    ax.plot(val_accs, label = \"validation\")\n    ax.legend()\n    fig.tight_layout()\n    \n    print(\"Final accuracy (train):\\t\\t{:.3f}\".format(train_accs[-1]))\n    print(\"Final accuracy (validation):\\t{:.3f}\\n\".format(val_accs[-1]))\n\ndef train_wnd_classifier(periods, dnn_learning_rate, lin_learning_rate, steps, batch_size, hidden_units, train_ex, train_targ, val_ex, val_targ, use_age=False):\n    #steps per period (spp)\n    spp = steps / periods\n    # We'll use the FTRL optimizer for our linear portion\n    lin_optim = tf.train.FtrlOptimizer(learning_rate = lin_learning_rate)\n    # We'll use the ProximalAdagradOptimizer with L1 regularization to punish overly complex deep models\n    # We'll use L2 regularization to punish over-reliance on any one feature\n    dnn_optim = tf.train.ProximalAdagradOptimizer(learning_rate = dnn_learning_rate,\n                                                  l1_regularization_strength = 0.05,\n                                                  l2_regularization_strength = 0.05)\n\n    wide_features, deep_features = build_feature_columns(use_age)\n    # We'll use a wide-n-deep classifier to get model that can memorize and generalize\n    wnd_classifier = tf.estimator.DNNLinearCombinedClassifier(\n        #Wide settings\n        linear_feature_columns = wide_features,\n        linear_optimizer = lin_optim,\n        #Deep settings\n        dnn_feature_columns = deep_features,\n        dnn_hidden_units = hidden_units,\n        dnn_optimizer = dnn_optim,\n        dnn_dropout = 0.5,\n        dnn_activation_fn = tf.nn.leaky_relu)\n    \n    # Input functions\n    train_input_fn = lambda: input_fn(train_ex, train_targ, batch_size = batch_size)\n    pred_train_input_fn = lambda: input_fn(train_ex, train_targ, num_epochs = 1, shuffle = False)\n    pred_val_input_fn = lambda: input_fn(val_ex, val_targ, num_epochs = 1, shuffle = False)\n    #train and validation accuracy per period\n    train_app = []\n    val_app = []\n    for period in range(periods):\n        # Train our classifier\n        wnd_classifier.train(input_fn = train_input_fn, steps = spp)\n        # Check how our classifier does on training set after one period\n        train_pred = wnd_classifier.predict(input_fn = pred_train_input_fn)\n        train_pred = np.array([pred['class_ids'][0] for pred in train_pred])\n        # Check how our classifier does on the validation set after one period\n        val_pred = wnd_classifier.predict(input_fn = pred_val_input_fn)\n        val_pred = np.array([pred['class_ids'][0] for pred in val_pred])\n        # Calculate accuracy metrics\n        train_acc = accuracy_score(train_targ, train_pred)\n        val_acc = accuracy_score(val_targ, val_pred)\n        print(\"period {} train acc: {:.3f}\".format(str(period).zfill(3), train_acc))\n        # Add our accuracies to running list\n        train_app.append(train_acc)\n        val_app.append(val_acc)\n    print(\"\\nTraining done!\\n\")\n    plot_acc(train_app, val_app)\n    return (wnd_classifier, train_app, val_app)","execution_count":95,"outputs":[]},{"metadata":{"_cell_guid":"0df7c3eb-4af1-49a3-b24f-57fd414d7132","_uuid":"e0029b3e5acefa3019617a0b1ed26a70b25628d2"},"cell_type":"markdown","source":"---"},{"metadata":{"_cell_guid":"32234ac2-5cdd-40b4-a920-91974ed49f31","_uuid":"3299f0ec38fb315587a06103874f195e4c9e073f"},"cell_type":"markdown","source":"# 6) Cross-validation of our model"},{"metadata":{"_cell_guid":"ce32ee9c-f624-4a20-892c-5585267d1cc6","_uuid":"c130c41d191855b92d52c93a9d81a5e3b02db435"},"cell_type":"markdown","source":"Selecting our features to use..."},{"metadata":{"_cell_guid":"9fa09a56-dda9-4a1f-81ec-f4e9f95e8878","collapsed":true,"_uuid":"d5617c0271a7086d478e9b22510e7b4baf52cf40","trusted":true},"cell_type":"code","source":"BASIC_MODEL = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"Ticket\", \"family_size\", \"title\"]\nAGE_MODEL = BASIC_MODEL + [\"Age\"]\n# Unused features:\n# deck_level\n# Cabin","execution_count":96,"outputs":[]},{"metadata":{"_cell_guid":"f8c9f8de-567d-4119-8cb5-49315f78b321","_uuid":"cf36910c1bd353b0976daa3c20ba4aedc9434429"},"cell_type":"markdown","source":"We'll use K-Fold cross validation to get an idea of how our models will perform on the test data."},{"metadata":{"scrolled":false,"_cell_guid":"41bcf882-bdda-458b-ace5-b42e2055e8f9","collapsed":true,"_uuid":"6a30398e2d94bd085f0faf9be8d59cb4741bfccb","trusted":true},"cell_type":"code","source":"def run_kfold_analysis(train_df, use_age=False):\n    kf = KFold(n_splits = 10, shuffle = True)\n    if (use_age):\n        # If we're training our age model then we need to drop all rows where 'Age' == NaN\n        train_df = train_df.dropna(axis = 0)\n        features_to_use = AGE_MODEL\n    else:\n        features_to_use = BASIC_MODEL\n    split_indices = kf.split(train_df)\n\n    fold_classifiers = []\n    for curr_fold, (train_indices, xval_indices) in enumerate(split_indices):\n        # Set up our train examples and targets\n        train_ex_df = train_df.iloc[train_indices]\n        train_targ_series = train_ex_df['Survived']\n        # Set up our xval examples and targets\n        xval_ex_df = train_df.iloc[xval_indices]\n        xval_targ_series = xval_ex_df['Survived']\n        # Select our variables of interest\n        train_ex_df = train_ex_df[features_to_use]\n        xval_ex_df = xval_ex_df[features_to_use]\n\n        print(\"========= Current fold: {} =============\".format(curr_fold))\n        print(\"\\nTotal training samples: {}\".format(len(train_df)))\n        print(\"New training split: {}\".format(len(train_ex_df)))\n        print(\"New xval split: {}\\n\".format(len(xval_ex_df)))\n\n        classifier, train_perf, val_perf = train_wnd_classifier(periods = 10,\n                                                                lin_learning_rate = 0.01,\n                                                                dnn_learning_rate = 0.01,\n                                                                steps = 6000,\n                                                                batch_size = 15,\n                                                                hidden_units = [200, 150, 100],\n                                                                train_ex = train_ex_df,\n                                                                train_targ = train_targ_series,\n                                                                val_ex = xval_ex_df,\n                                                                val_targ = xval_targ_series,\n                                                                use_age = use_age)\n        \n        fold_classifiers.append((classifier, train_perf, val_perf))\n    return(fold_classifiers)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1f0440c1-b8cb-462f-aacf-c246ffaf4829","_uuid":"f88a52bc30fd4b7bf7b0365dc72e4111d0e76ea9"},"cell_type":"markdown","source":"### Uncomment the following lines to run cross_validation analysis (It's time consuming though!)"},{"metadata":{"_cell_guid":"d2cfd8be-87da-47bd-99a8-642e7148b034","_uuid":"7f6bb823433f0d8f5641d4a0b77a508d4b820bea","trusted":true},"cell_type":"code","source":"print(\"\\n================= Results of BASIC_MODEL ==================\\n\")\nbasic_model_results = run_kfold_analysis(train_df, use_age = False)\nprint(\"\\n================== Results of AGE_MODEL ===================\\n\")\nage_model_results = run_kfold_analysis(train_df, use_age = True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"728dbbc6-28ae-4192-9d61-39e2e0339d4a","_uuid":"e6f41444ead982ef7f41786b648c0e3bca5f52d2"},"cell_type":"markdown","source":"# 7) Train on our entire dataset and make predictions for test dataset"},{"metadata":{"_cell_guid":"db9a1cef-cdde-4b6c-b6fd-260c22355aa8","_uuid":"b510d1fd5cb4dfa221a3b8ab0bcbddcd935bc9b7"},"cell_type":"markdown","source":"### Train our basic model"},{"metadata":{"_cell_guid":"016c09e7-40f2-499c-a9df-86c4afaa38f9","collapsed":true,"_uuid":"e2b55b8b9f2e1a64ef2044c993f821420b2be80f","trusted":true},"cell_type":"code","source":"# Set up our train examples and targets\nfinal_train_ex_df = train_df\nfinal_train_targ_series = final_train_ex_df['Survived']\n# Dummy dataset that we won't actually consider or care about\ndummy_df = final_train_ex_df\ndummy_targ_series = final_train_targ_series\n# Select features in our basic model\nfinal_train_ex_df = final_train_ex_df[BASIC_MODEL]\ndummy_df = dummy_df[BASIC_MODEL]\n\nbasic_classifier, basic_train_perf, _ = train_wnd_classifier(periods = 10,\n                                                            lin_learning_rate = 0.01,\n                                                            dnn_learning_rate = 0.01,\n                                                            steps = 6000,\n                                                            batch_size = 15,\n                                                            hidden_units = [200, 150, 100],\n                                                            train_ex = final_train_ex_df,\n                                                            train_targ = final_train_targ_series,\n                                                            val_ex = dummy_df,\n                                                            val_targ = dummy_targ_series,\n                                                            use_age = False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8850c456-6ac3-4383-8205-d3a707bd32c3","_uuid":"715b0b9f11a579ca18e825fc6b75f19c27aef8b4"},"cell_type":"markdown","source":"### Train our age model"},{"metadata":{"_cell_guid":"12becfea-3f1c-4138-8c6a-b66d2108c4ff","collapsed":true,"_uuid":"fd204a7285800e4bb85442b8a2842af6fd5a0be7","trusted":true},"cell_type":"code","source":"# Set up our train examples and targets\nfinal_train_ex_df = train_df.dropna(axis = 0)\nfinal_train_targ_series = final_train_ex_df['Survived']\n# Dummy dataset that we won't actually consider or care about\ndummy_df = final_train_ex_df\ndummy_targ_series = final_train_targ_series\n# Select features in our age model\nfinal_train_ex_df = final_train_ex_df[AGE_MODEL]\ndummy_df = dummy_df[AGE_MODEL]\n\nage_classifier, age_train_perf, _ = train_wnd_classifier(periods = 10,\n                                                         lin_learning_rate = 0.01,\n                                                         dnn_learning_rate = 0.01,\n                                                         steps = 6000,\n                                                         batch_size = 15,\n                                                         hidden_units = [200, 150, 100],\n                                                         train_ex = final_train_ex_df,\n                                                         train_targ = final_train_targ_series,\n                                                         val_ex = dummy_df,\n                                                         val_targ = dummy_targ_series,\n                                                         use_age = True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"382b73a2-4e0e-4c24-9ef4-99e79d866c37","_uuid":"d29a00d5581d62032b752305b7ecfb17be62ecec"},"cell_type":"markdown","source":"### Make predictions for the test set"},{"metadata":{"_cell_guid":"0934980a-3f0d-455c-8859-aed3612251ee","collapsed":true,"_uuid":"25d0f24f72e39aa1c60b831d23071329e4930191","trusted":true},"cell_type":"code","source":"# Split our test set into 2 dataframes (one where we can use our age model and the other where we use our basic model)\nage_test_ex_df = test_df.dropna(axis = 0)\nbasic_test_ex_df = test_df.drop(age_test_ex_df.index)\n\n# Create a dummy series that will be compatible with our input_fn\nage_test_targ_series = pd.Series(np.zeros(len(age_test_ex_df), dtype=int))\nbasic_test_targ_series = pd.Series(np.zeros(len(basic_test_ex_df), dtype=int))\n\n# Setup our input functions\npred_age_test_input_fn = lambda: input_fn(age_test_ex_df[AGE_MODEL], age_test_targ_series, num_epochs = 1, shuffle = False)\npred_basic_test_input_fn = lambda: input_fn(basic_test_ex_df[BASIC_MODEL], basic_test_targ_series, num_epochs = 1, shuffle = False)\n\n# Make predictions for rows that have ages\nage_test_preds = age_classifier.predict(input_fn = pred_age_test_input_fn)\nage_test_preds = np.array([pred['class_ids'][0] for pred in age_test_preds])\n\n# Make predictions for rows that lack ages\nbasic_test_preds = basic_classifier.predict(input_fn = pred_basic_test_input_fn)\nbasic_test_preds = np.array([pred['class_ids'][0] for pred in basic_test_preds])\n\n# Let's put together our submission dataframe by merging our age and basic prediction sets\nage_predictions = age_test_ex_df[[\"PassengerId\"]]\nage_predictions = age_predictions.assign(Survived=pd.Series(age_test_preds).values)\nbasic_predictions = basic_test_ex_df[[\"PassengerId\"]]\nbasic_predictions = basic_predictions.assign(Survived=pd.Series(basic_test_preds).values)\nsubmission_df = pd.concat([age_predictions, basic_predictions])\nsubmission_df = submission_df.sort_values(by=['PassengerId'])\ndisplay(submission_df)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ad7c919b-7bca-450d-a7b8-2404575b5b90","collapsed":true,"_uuid":"99773f12efde28697e32952981b774c7fb28223e","trusted":true},"cell_type":"code","source":"submission_df.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72200c0a9cff1a38e2a946f1469bd8c13b101197"},"cell_type":"markdown","source":"# Closing thoughts:\n\n* I definitely should have started out with a simple set of features (i.e. Pclass, Sex, Fare) and then built up because I wasted a lot of early submissions with an overly complex model. This made it extremely hard to troubleshoot things. In the future, I will likely start with a simple set of features then add on new features once I've really optimized the hyperparameters for the more basic set.\n\n* I found in general that having high cross-validation accuracy as well as small variance between cross-validation scores for each K-fold analysis was a somewhat good predictor for reasonable test performance.\n\n* It's defintely possible that I got lucky with my submissions. The real test would be seeing how well it generalizes on the held-out test portion of the Titanic test data.\n\n* I'm still learning a lot so feedback and new suggestions would definitely be appreciated!"}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}